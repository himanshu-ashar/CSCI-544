{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187aebea",
   "metadata": {},
   "source": [
    "## HW4 - PDF Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8de04e",
   "metadata": {},
   "source": [
    "In order to explain my answers to task 1 and task 2 more efficiently, I have submitted my Jupyter notebook as a PDF, so that I can explain my answers in markdown cells with corresponding code directly backing my explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58ee5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3dd5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/train\",\"r\")\n",
    "count_dict = defaultdict(int)\n",
    "label_set = []\n",
    "for line in f:\n",
    "    get_words = line.split()\n",
    "    if len(get_words)!=0:\n",
    "        count_dict[get_words[1]]+=1\n",
    "        if get_words[2] not in label_set:\n",
    "            label_set.append(get_words[2])\n",
    "f.close()\n",
    "\n",
    "unkw = 0\n",
    "for key,val in count_dict.items():\n",
    "    if val<2:\n",
    "        unkw += val\n",
    "\n",
    "sorted_count_list = sorted(count_dict.items(),key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ce2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<UNK>'] = 1\n",
    "\n",
    "i=2\n",
    "for word,count in sorted_count_list:\n",
    "    if count>=2:\n",
    "        word_index[word] = i\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1958b7f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11985"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b23f23",
   "metadata": {},
   "source": [
    "#### Above is the custom vocab index created for task 1, with 11985 items, including padding and unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77e6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open(\"./data/train\",\"r\")\n",
    "sentences = []\n",
    "tags = []\n",
    "curr_sent = \"\"\n",
    "curr_tags = \"\"\n",
    "\n",
    "for line in f_train:\n",
    "    get_line = line.split()\n",
    "    if len(get_line)>0:\n",
    "        curr_sent += get_line[1]\n",
    "        curr_sent += \" \"\n",
    "        curr_tags += get_line[2]\n",
    "        curr_tags += \" \"\n",
    "    else:\n",
    "        curr_sent = curr_sent[:-1]\n",
    "        curr_tags = curr_tags[:-1]\n",
    "        sentences.append(curr_sent)\n",
    "        tags.append(curr_tags)\n",
    "        curr_sent = \"\"\n",
    "        curr_tags = \"\"\n",
    "f_train.close()\n",
    "\n",
    "curr_sent = curr_sent[:-1]\n",
    "curr_tags = curr_tags[:-1]\n",
    "sentences.append(curr_sent)\n",
    "tags.append(curr_tags)\n",
    "curr_sent = \"\"\n",
    "curr_tags = \"\"\n",
    "\n",
    "train_data = pd.DataFrame({'sentences':sentences, 'tags':tags})\n",
    "\n",
    "f_dev = open(\"./data/dev\",\"r\")\n",
    "sentences = []\n",
    "tags = []\n",
    "curr_sent = \"\"\n",
    "curr_tags = \"\"\n",
    "\n",
    "for line in f_dev:\n",
    "    get_line = line.split()\n",
    "    if len(get_line)>0:\n",
    "        curr_sent += get_line[1]\n",
    "        curr_sent += \" \"\n",
    "        curr_tags += get_line[2]\n",
    "        curr_tags += \" \"\n",
    "    else:\n",
    "        curr_sent = curr_sent[:-1]\n",
    "        curr_tags = curr_tags[:-1]\n",
    "        sentences.append(curr_sent)\n",
    "        tags.append(curr_tags)\n",
    "        curr_sent = \"\"\n",
    "        curr_tags = \"\"\n",
    "f_dev.close()\n",
    "\n",
    "curr_sent = curr_sent[:-1]\n",
    "curr_tags = curr_tags[:-1]\n",
    "sentences.append(curr_sent)\n",
    "tags.append(curr_tags)\n",
    "curr_sent = \"\"\n",
    "curr_tags = \"\"\n",
    "\n",
    "dev_data = pd.DataFrame({'sentences':sentences, 'tags':tags})\n",
    "\n",
    "f_test = open(\"./data/test\",\"r\")\n",
    "sentences = []\n",
    "# tags = []\n",
    "curr_sent = \"\"\n",
    "# curr_tags = \"\"\n",
    "\n",
    "for line in f_test:\n",
    "    get_line = line.split()\n",
    "    if len(get_line)>0:\n",
    "        curr_sent += get_line[1]\n",
    "        curr_sent += \" \"\n",
    "#         curr_tags += get_line[2]\n",
    "#         curr_tags += \" \"\n",
    "    else:\n",
    "        curr_sent = curr_sent[:-1]\n",
    "#         curr_tags = curr_tags[:-1]\n",
    "        sentences.append(curr_sent)\n",
    "#         tags.append(curr_tags)\n",
    "        curr_sent = \"\"\n",
    "#         curr_tags = \"\"\n",
    "f_test.close()\n",
    "\n",
    "curr_sent = curr_sent[:-1]\n",
    "# curr_tags = curr_tags[:-1]\n",
    "sentences.append(curr_sent)\n",
    "# tags.append(curr_tags)\n",
    "curr_sent = \"\"\n",
    "# curr_tags = \"\"\n",
    "\n",
    "test_data = pd.DataFrame({'sentences':sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a50841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EU rejects German call to boycott British lamb .</td>\n",
       "      <td>B-ORG O B-MISC O O O B-MISC O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>B-PER I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BRUSSELS 1996-08-22</td>\n",
       "      <td>B-LOC O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The European Commission said on Thursday it di...</td>\n",
       "      <td>O B-ORG I-ORG O O O O O O B-MISC O O O O O B-M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany 's representative to the European Unio...</td>\n",
       "      <td>B-LOC O O O O B-ORG I-ORG O O O B-PER I-PER O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>Division two</td>\n",
       "      <td>O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>Plymouth 2 Preston 1</td>\n",
       "      <td>B-ORG O B-ORG O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>Division three</td>\n",
       "      <td>O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>Swansea 1 Lincoln 2</td>\n",
       "      <td>B-ORG O B-ORG O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>-DOCSTART-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14987 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  \\\n",
       "0       EU rejects German call to boycott British lamb .   \n",
       "1                                        Peter Blackburn   \n",
       "2                                    BRUSSELS 1996-08-22   \n",
       "3      The European Commission said on Thursday it di...   \n",
       "4      Germany 's representative to the European Unio...   \n",
       "...                                                  ...   \n",
       "14982                                       Division two   \n",
       "14983                               Plymouth 2 Preston 1   \n",
       "14984                                     Division three   \n",
       "14985                                Swansea 1 Lincoln 2   \n",
       "14986                                         -DOCSTART-   \n",
       "\n",
       "                                                    tags  \n",
       "0                        B-ORG O B-MISC O O O B-MISC O O  \n",
       "1                                            B-PER I-PER  \n",
       "2                                                B-LOC O  \n",
       "3      O B-ORG I-ORG O O O O O O B-MISC O O O O O B-M...  \n",
       "4      B-LOC O O O O B-ORG I-ORG O O O B-PER I-PER O ...  \n",
       "...                                                  ...  \n",
       "14982                                                O O  \n",
       "14983                                    B-ORG O B-ORG O  \n",
       "14984                                                O O  \n",
       "14985                                    B-ORG O B-ORG O  \n",
       "14986                                                  O  \n",
       "\n",
       "[14987 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab84a61",
   "metadata": {},
   "source": [
    "Above, I have created dataframes for each: train, dev and test set. This will be helpful in passing data to the Dataset creator and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617a87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "i=0\n",
    "for label in label_set:\n",
    "    label_index[label] = i\n",
    "    i+=1\n",
    "label_index['pad_label'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034f7ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ORG': 0,\n",
       " 'O': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-LOC': 8,\n",
       " 'pad_label': -1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c3b7c",
   "metadata": {},
   "source": [
    "Above is the label to index mapper to compute output labels in the model.\n",
    "\n",
    "Below are the reverse index to label and word mappers to make deductions when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31db6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {v: k for k, v in word_index.items()}\n",
    "index_label = {v: k for k, v in label_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23f727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataBiLSTM:\n",
    "    def __init__(self, sentences, tags, word_index, label_index):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_index = word_index\n",
    "        self.label_index = label_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sentence = self.sentences.iloc[i].split()\n",
    "        ner_tag = self.tags.iloc[i].split()\n",
    "        \n",
    "        sentence = [self.word_index.get(word, self.word_index['<UNK>']) for word in sentence]\n",
    "        ner_tag = [self.label_index[tag] for tag in ner_tag]\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        ner_tag = torch.tensor(ner_tag)\n",
    "        \n",
    "        return sentence, ner_tag\n",
    "\n",
    "def pad_collate(batch):\n",
    "#     batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    sentences, ner_tags = zip(*batch)\n",
    "    \n",
    "    lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "    sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    ner_tags = pad_sequence(ner_tags, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return sentences, lengths, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae595ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevDataBiLSTM:\n",
    "    def __init__(self, sentences, tags, word_index, label_index):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_index = word_index\n",
    "        self.label_index = label_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sentence = self.sentences.iloc[i].split()\n",
    "        ner_tag = self.tags.iloc[i].split()\n",
    "        \n",
    "        sentence = [self.word_index.get(word, self.word_index['<UNK>']) for word in sentence]\n",
    "        ner_tag = [self.label_index[tag] for tag in ner_tag]\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        ner_tag = torch.tensor(ner_tag)\n",
    "        \n",
    "        return sentence, ner_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777c3c2",
   "metadata": {},
   "source": [
    "#### The data loaders for the train and test data are identical. To pad each batch, such that each sentence attains the length of longest sentence of that batch by padding with zeros, I have created a pad_collate function, which will also return original lengths of sentences. This will be passed to DataLoader to get padded data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e77dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataBiLSTM:\n",
    "    def __init__(self, sentences, word_index):\n",
    "        self.sentences = sentences\n",
    "#         self.tags = tags\n",
    "        self.word_index = word_index\n",
    "#         self.label_index = label_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sentence = self.sentences.iloc[i].split()\n",
    "#         ner_tag = self.tags.iloc[i].split()\n",
    "        \n",
    "        sentence = [self.word_index.get(word, self.word_index['<UNK>']) for word in sentence]\n",
    "#         ner_tag = [self.label_index[tag] for tag in ner_tag]\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "#         ner_tag = torch.tensor(ner_tag)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "def pad_collate_test(batch):\n",
    "#     batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    sentences = batch\n",
    "    \n",
    "    lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "    sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     ner_tags = pad_sequence(ner_tags, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return sentences, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec21d72",
   "metadata": {},
   "source": [
    "The test dataset is created in a similar manner, except it does not have actual tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42929ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "train_dataset = TrainDataBiLSTM(train_data['sentences'], train_data['tags'], word_index, label_index)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=pad_collate)\n",
    "\n",
    "dev_dataset = DevDataBiLSTM(dev_data['sentences'], dev_data['tags'], word_index, label_index)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=pad_collate)\n",
    "\n",
    "test_dataset = TestDataBiLSTM(test_data['sentences'], word_index)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=pad_collate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e57b17",
   "metadata": {},
   "source": [
    "### Please note above that my batch size is 16.\n",
    "\n",
    "#### I have chosen this as my batch size hyperparameter value after trying out different batch sizes like 32, 64, etc. but 16 gives the fastest convergence to produce the expected result on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0594f2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRI...\n",
       "1                                             Nadim Ladki\n",
       "2                AL-AIN , United Arab Emirates 1996-12-06\n",
       "3       Japan began the defence of their Asian Cup tit...\n",
       "4       But China saw their luck desert them in the se...\n",
       "                              ...                        \n",
       "3679    \" It was the joy that we all had over the peri...\n",
       "3680    Charlton managed Ireland for 93 matches , duri...\n",
       "3681    He guided Ireland to two successive World Cup ...\n",
       "3682    The lanky former Leeds United defender did not...\n",
       "3683                                           -DOCSTART-\n",
       "Name: sentences, Length: 3684, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4937fb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe0ae2",
   "metadata": {},
   "source": [
    "### Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37848c1",
   "metadata": {},
   "source": [
    "#### My Simple LSTM model is defined below:\n",
    "\n",
    "The fixed hyper-parameters as per the assignment requirement are:\n",
    "\n",
    "Embedding dimension: 100,\n",
    "\n",
    "LSTM hidden dim: 256,\n",
    "\n",
    "LSTM dropout: 0.33,\n",
    "\n",
    "LSTM layers: 1,\n",
    "\n",
    "Linear output dim: 128,\n",
    "\n",
    "ELU layer: Here, I have determined alpha=0.35 gives a slight boost in results.\n",
    "\n",
    "Classifier layer: 9 outputs (corresponding to tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "426122f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(11985, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=0.35)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, lstm_dropout, linear_output_dim, num_tags):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_dim, num_layers=1, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim*2, linear_output_dim)\n",
    "        self.elu = nn.ELU(0.35)\n",
    "        self.classifier = nn.Linear(linear_output_dim, num_tags)\n",
    "    \n",
    "    def forward(self, inputs, lengths):\n",
    "        embedded = self.embedding(inputs)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.dropout(output)\n",
    "        linear_output = self.linear(output)\n",
    "        elu_output = self.elu(linear_output)\n",
    "        logits = self.classifier(elu_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "bilstm_model = BiLSTM(len(word_index.keys()), 100, 256, 0.33, 128, 9)\n",
    "print(bilstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf2808",
   "metadata": {},
   "source": [
    "11985 corresponds to custom created vocabulary data size from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d39346f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.SGD(bilstm_model.parameters(), lr=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b189a41",
   "metadata": {},
   "source": [
    "As required, SGD optimizer is used. Trying out various learning rates and epochs, I determined that a learning rate = 0.33, and epochs=50 gave best convergence and promising results on the dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8abe369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.678128\n",
      "Epoch: 2 \tTraining Loss: 0.466299\n",
      "Epoch: 3 \tTraining Loss: 0.353567\n",
      "Epoch: 4 \tTraining Loss: 0.274990\n",
      "Epoch: 5 \tTraining Loss: 0.220591\n",
      "Epoch: 6 \tTraining Loss: 0.185073\n",
      "Epoch: 7 \tTraining Loss: 0.157392\n",
      "Epoch: 8 \tTraining Loss: 0.135189\n",
      "Epoch: 9 \tTraining Loss: 0.118245\n",
      "Epoch: 10 \tTraining Loss: 0.107091\n",
      "Epoch: 11 \tTraining Loss: 0.094217\n",
      "Epoch: 12 \tTraining Loss: 0.084123\n",
      "Epoch: 13 \tTraining Loss: 0.078213\n",
      "Epoch: 14 \tTraining Loss: 0.068511\n",
      "Epoch: 15 \tTraining Loss: 0.062229\n",
      "Epoch: 16 \tTraining Loss: 0.058264\n",
      "Epoch: 17 \tTraining Loss: 0.052831\n",
      "Epoch: 18 \tTraining Loss: 0.049279\n",
      "Epoch: 19 \tTraining Loss: 0.045459\n",
      "Epoch: 20 \tTraining Loss: 0.041738\n",
      "Epoch: 21 \tTraining Loss: 0.038185\n",
      "Epoch: 22 \tTraining Loss: 0.035769\n",
      "Epoch: 23 \tTraining Loss: 0.033005\n",
      "Epoch: 24 \tTraining Loss: 0.031737\n",
      "Epoch: 25 \tTraining Loss: 0.030521\n",
      "Epoch: 26 \tTraining Loss: 0.027203\n",
      "Epoch: 27 \tTraining Loss: 0.026357\n",
      "Epoch: 28 \tTraining Loss: 0.024640\n",
      "Epoch: 29 \tTraining Loss: 0.024221\n",
      "Epoch: 30 \tTraining Loss: 0.021627\n",
      "Epoch: 31 \tTraining Loss: 0.020940\n",
      "Epoch: 32 \tTraining Loss: 0.019803\n",
      "Epoch: 33 \tTraining Loss: 0.019019\n",
      "Epoch: 34 \tTraining Loss: 0.017873\n",
      "Epoch: 35 \tTraining Loss: 0.017430\n",
      "Epoch: 36 \tTraining Loss: 0.016243\n",
      "Epoch: 37 \tTraining Loss: 0.015551\n",
      "Epoch: 38 \tTraining Loss: 0.014719\n",
      "Epoch: 39 \tTraining Loss: 0.014044\n",
      "Epoch: 40 \tTraining Loss: 0.013615\n",
      "Epoch: 41 \tTraining Loss: 0.013524\n",
      "Epoch: 42 \tTraining Loss: 0.011710\n",
      "Epoch: 43 \tTraining Loss: 0.011689\n",
      "Epoch: 44 \tTraining Loss: 0.012621\n",
      "Epoch: 45 \tTraining Loss: 0.011514\n",
      "Epoch: 46 \tTraining Loss: 0.010876\n",
      "Epoch: 47 \tTraining Loss: 0.010281\n",
      "Epoch: 48 \tTraining Loss: 0.009557\n",
      "Epoch: 49 \tTraining Loss: 0.009951\n",
      "Epoch: 50 \tTraining Loss: 0.009323\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "validn_min_loss = np.Inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    bilstm_model.train()\n",
    "    for sentences, lengths, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = bilstm_model(sentences, lengths)\n",
    "        output = output.permute(0,2,1)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*sentences.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_loader.dataset))\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c151099",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bilstm_model.state_dict(), 'bilstm_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98df0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilstm_model = BiLSTM(len(word_index.keys()), 100, 256, 0.33, 128, 9)\n",
    "# bilstm_model.load_state_dict(torch.load('bilstm_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b0f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevResults(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    f_read = open(\"./data/dev\",\"r\")\n",
    "    f_write = open(\"dev1.out\",\"w\")\n",
    "    for sentences, lengths, labels in dataloader:\n",
    "        output = model(sentences, lengths)\n",
    "        max_values, max_indices = torch.max(output, dim=2)\n",
    "        y = max_indices\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                read_line = f_read.readline().split()\n",
    "                if len(read_line)>0:\n",
    "                    f_write.write(str(read_line[0])+\" \"+str(read_line[1])+\" \"+index_label[labels[i][j].item()]+\" \"+index_label[y[i][j].item()]+\"\\n\")\n",
    "                else:\n",
    "                    break\n",
    "                if j+1>=len(sentences[i]):\n",
    "                    f_read.readline()\n",
    "            if len(sentences)==batch_size or i<len(sentences)-1:\n",
    "                f_write.write(\"\\n\")\n",
    "    f_read.close()\n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1965ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "getDevResults(bilstm_model, dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b06477",
   "metadata": {},
   "source": [
    "#### The dev results are as follows:            \n",
    "              \n",
    "              \n",
    "    processed 51578 tokens with 5942 phrases; found: 5453 phrases; correct: 4468.\n",
    "\n",
    "    accuracy:  95.69%; precision:  81.94%; recall:  75.19%; FB1:  78.42\n",
    "\n",
    "              LOC: precision:  89.39%; recall:  83.45%; FB1:  86.32  1715\n",
    "              \n",
    "             MISC: precision:  80.81%; recall:  75.81%; FB1:  78.23  865\n",
    "             \n",
    "              ORG: precision:  75.31%; recall:  67.56%; FB1:  71.23  1203\n",
    "              \n",
    "              PER: precision:  79.64%; recall:  72.20%; FB1:  75.74  1670\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17acf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestResults(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    f_read = open(\"./data/test\",\"r\")\n",
    "    f_write = open(\"test1.out\",\"w\")\n",
    "    for sentences, lengths in dataloader:\n",
    "        output = model(sentences, lengths)\n",
    "        max_values, max_indices = torch.max(output, dim=2)\n",
    "        y = max_indices\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                read_line = f_read.readline().split()\n",
    "                if len(read_line)>0:\n",
    "                    f_write.write(str(read_line[0])+\" \"+str(read_line[1])+\" \"+index_label[y[i][j].item()]+\"\\n\")\n",
    "                else:\n",
    "                    break\n",
    "                if j+1>=len(sentences[i]):\n",
    "                    f_read.readline()\n",
    "            if len(sentences)==batch_size or i<len(sentences)-1:\n",
    "                f_write.write(\"\\n\")\n",
    "    f_read.close()\n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf768f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "getTestResults(bilstm_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1273eb",
   "metadata": {},
   "source": [
    "### Task 2: Bi-directional LSTM model with GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b4db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_vectors = []\n",
    "embed_vocab = []\n",
    "file_embed = open(\"glove.6B.100d\",\"r\")\n",
    "for line in file_embed:\n",
    "    line = line.split()\n",
    "    embed_vocab.append(line[0])\n",
    "    embed_vectors.append(line[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "915bd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_vocab = np.array(embed_vocab)\n",
    "embed_vectors = np.array(embed_vectors, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8dc7b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6daa490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c530b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_vector = np.zeros((1,embed_vectors.shape[1]))\n",
    "unk_vector = np.mean(embed_vectors,axis=0,keepdims=True)\n",
    "\n",
    "embed_vocab = np.insert(embed_vocab, 0, '<PAD>')\n",
    "embed_vocab = np.insert(embed_vocab, 1, '<UNK>')\n",
    "\n",
    "embed_vectors = np.vstack((pad_vector,unk_vector,embed_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35270252",
   "metadata": {},
   "source": [
    "#### Above, word embeddings from GloVe are loaded into vectors. Also, I have added vectors for the padding and unknown token.\n",
    "\n",
    "Experimentally, I found that assigned 100 dim embeddings of zero for padding token, and average of all available glove word embeddings to unknown token, gave best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bba9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_embed = nn.Embedding.from_pretrained(torch.from_numpy(embed_vectors),padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "963d0207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
       "         -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
       "          0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
       "          0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
       "          0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
       "         -0.7179, -0.4153,  0.2034, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
       "         -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9978, -0.8048, -3.0243,\n",
       "          0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
       "          1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
       "         -0.3080, -0.4163,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
       "          0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
       "          0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
       "         -0.5203, -0.1459,  0.8278,  0.2706]], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_embed(torch.LongTensor([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b312af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vectors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ea2a861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<PAD>', '<UNK>', 'the', ..., 'rolonda', 'zsombor', 'sandberger'],\n",
       "      dtype='<U68')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de085bfe",
   "metadata": {},
   "source": [
    "Above was some sample code to demonstrate loaded glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "469fcd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_word_index = dict(zip(embed_vocab, embed_vectors))\n",
    "glove_word_index = {k: v for v, k in enumerate(embed_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bdd88",
   "metadata": {},
   "source": [
    "#### The way train, test and dev datasets are created is similar to task 1, but has some differences.\n",
    "\n",
    "#### GloVe only has lowercase word embeddings. Hence, to retrieve word embeddings, each word's lowercase form is taken to access corresponding glove index. \n",
    "\n",
    "#### Also, an extra vector of is_capitals is passed, which contains 1 if the word is either first word capitlaized or full capitalized, and 0 if the word is completely lowercase. This vector will be concatenated to output of embedding layer in forward pass, and then passed to Bi-LSTLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d97727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataBiLSTMGlove:\n",
    "    def __init__(self, sentences, tags, glove_word_index, label_index):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.glove_word_index = glove_word_index\n",
    "        self.label_index = label_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sentence = self.sentences.iloc[i].split()\n",
    "        ner_tag = self.tags.iloc[i].split()\n",
    "        \n",
    "        is_capital = [1 if (word.isupper() or word.istitle()) else 0 for word in sentence]\n",
    "        sentence = [self.glove_word_index.get(word.lower(), self.glove_word_index['<UNK>']) for word in sentence]\n",
    "        ner_tag = [self.label_index[tag] for tag in ner_tag]\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        is_capital = torch.tensor(is_capital)\n",
    "        ner_tag = torch.tensor(ner_tag)\n",
    "        \n",
    "        return sentence, is_capital, ner_tag\n",
    "\n",
    "def pad_collate_glove(batch):\n",
    "    \n",
    "    sentences, is_capitals, ner_tags = zip(*batch)\n",
    "    \n",
    "    lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "    sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
    "    is_capitals = pad_sequence(is_capitals, batch_first=True, padding_value=-1)\n",
    "    ner_tags = pad_sequence(ner_tags, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return sentences, is_capitals, lengths, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9869c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevDataBiLSTMGlove:\n",
    "    def __init__(self, sentences, tags, glove_word_index, label_index):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.glove_word_index = glove_word_index\n",
    "        self.label_index = label_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sentence = self.sentences.iloc[i].split()\n",
    "        ner_tag = self.tags.iloc[i].split()\n",
    "        \n",
    "        is_capital = [1 if (word.isupper() or word.istitle()) else 0 for word in sentence]\n",
    "        sentence = [self.glove_word_index.get(word.lower(), self.glove_word_index['<UNK>']) for word in sentence]\n",
    "        ner_tag = [self.label_index[tag] for tag in ner_tag]\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        is_capital = torch.tensor(is_capital)\n",
    "        ner_tag = torch.tensor(ner_tag)\n",
    "        \n",
    "        return sentence, is_capital, ner_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45649317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataBiLSTMGlove:\n",
    "    def __init__(self, sentences, glove_word_index):\n",
    "        self.sentences = sentences\n",
    "#         self.tags = tags\n",
    "        self.glove_word_index = glove_word_index\n",
    "#         self.label_index = label_index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sentence = self.sentences.iloc[i].split()\n",
    "#         ner_tag = self.tags.iloc[i].split()\n",
    "        \n",
    "        is_capital = [1 if (word.isupper() or word.istitle()) else 0 for word in sentence]\n",
    "        sentence = [self.glove_word_index.get(word.lower(), self.glove_word_index['<UNK>']) for word in sentence]\n",
    "#         ner_tag = [self.label_index[tag] for tag in ner_tag]\n",
    "        \n",
    "        sentence = torch.tensor(sentence)\n",
    "        is_capital = torch.tensor(is_capital)\n",
    "#         ner_tag = torch.tensor(ner_tag)\n",
    "        \n",
    "        return sentence, is_capital\n",
    "    \n",
    "def pad_collate_glove_test(batch):\n",
    "#     batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    sentences, is_capitals = zip(*batch)\n",
    "    \n",
    "    lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "    sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
    "    is_capitals = pad_sequence(is_capitals, batch_first=True, padding_value=-1)\n",
    "#     ner_tags = pad_sequence(ner_tags, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return sentences, is_capitals, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76746fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataBiLSTMGlove(train_data['sentences'], train_data['tags'], glove_word_index, label_index)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=pad_collate_glove)\n",
    "\n",
    "dev_dataset = DevDataBiLSTMGlove(dev_data['sentences'], dev_data['tags'], glove_word_index, label_index)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=pad_collate_glove)\n",
    "\n",
    "test_dataset = TestDataBiLSTMGlove(test_data['sentences'], glove_word_index)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=pad_collate_glove_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5791b8a",
   "metadata": {},
   "source": [
    "#### The Bi-LSTM model with GloVe embeddings is defined below, with almost same hyperparameters as task 1, but a few changes which will be described below:\n",
    "\n",
    "The fixed hyper-parameters as per the assignment requirement are:\n",
    "\n",
    "Embedding dimension: 100,\n",
    "\n",
    "LSTM hidden dim: 256,\n",
    "\n",
    "LSTM dropout: 0.33,\n",
    "\n",
    "LSTM layers: 1,\n",
    "\n",
    "Linear output dim: 128,\n",
    "\n",
    "ELU layer: Here, alpha=1.0 unlike task 1.\n",
    "\n",
    "Classifier layer: 9 outputs (corresponding to tags)\n",
    "\n",
    "The optimizer used is again SGD as required.\n",
    "\n",
    "The learning rate is also same as task 1: 0.33\n",
    "\n",
    "Number of epochs: 50\n",
    "\n",
    "#### Changes are as follows:\n",
    "The embedding layer contains pre-trained GloVe embeddings. These are set to trainable, so will still keep learning throughout as the model runs epochs.\n",
    "The size is 400002, which indicates the 400000 words for which embeddings exist, plus the unknown and padding token embeddings (created and explained earlier).\n",
    "\n",
    "Also the input dim to LSTM will be 101, because the is_capitals tensor is also passed which has values 0 when corresponding word was all lowercase, and 1, when it had one(or more) letters capitalized.\n",
    "This is concatenated to the output from embedding layer, thus passing in input of size 100+1 to the Bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9609741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMGlove(\n",
      "  (embedding): Embedding(400002, 100, padding_idx=0)\n",
      "  (lstm): LSTM(101, 256, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMGlove(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_hidden_dim, lstm_dropout, linear_output_dim, num_tags):\n",
    "        super(BiLSTMGlove, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embed_vectors),padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim+1, hidden_size=lstm_hidden_dim, num_layers=1, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(lstm_dropout)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim*2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier = nn.Linear(linear_output_dim, num_tags)\n",
    "    \n",
    "    def forward(self, inputs, is_capitals, lengths):\n",
    "        embedded = self.embedding(inputs)\n",
    "        concatenated_tensor = torch.cat((embedded, is_capitals.unsqueeze(-1)), dim=-1)\n",
    "        packed_embedded = pack_padded_sequence(concatenated_tensor, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_embedded = packed_embedded.float()\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        linear_output = self.linear(output)\n",
    "        elu_output = self.elu(linear_output)\n",
    "        logits = self.classifier(elu_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "bilstm_glove_model = BiLSTMGlove(100, 256, 0.33, 128, 9)\n",
    "print(bilstm_glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09d0f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.SGD(bilstm_glove_model.parameters(), lr=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c0651e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.319138\n",
      "Epoch: 2 \tTraining Loss: 0.133306\n",
      "Epoch: 3 \tTraining Loss: 0.104646\n",
      "Epoch: 4 \tTraining Loss: 0.091157\n",
      "Epoch: 5 \tTraining Loss: 0.081219\n",
      "Epoch: 6 \tTraining Loss: 0.073103\n",
      "Epoch: 7 \tTraining Loss: 0.066240\n",
      "Epoch: 8 \tTraining Loss: 0.060333\n",
      "Epoch: 9 \tTraining Loss: 0.055168\n",
      "Epoch: 10 \tTraining Loss: 0.050570\n",
      "Epoch: 11 \tTraining Loss: 0.046397\n",
      "Epoch: 12 \tTraining Loss: 0.042540\n",
      "Epoch: 13 \tTraining Loss: 0.038943\n",
      "Epoch: 14 \tTraining Loss: 0.035568\n",
      "Epoch: 15 \tTraining Loss: 0.032413\n",
      "Epoch: 16 \tTraining Loss: 0.029915\n",
      "Epoch: 17 \tTraining Loss: 0.026851\n",
      "Epoch: 18 \tTraining Loss: 0.024158\n",
      "Epoch: 19 \tTraining Loss: 0.021704\n",
      "Epoch: 20 \tTraining Loss: 0.019351\n",
      "Epoch: 21 \tTraining Loss: 0.017327\n",
      "Epoch: 22 \tTraining Loss: 0.015248\n",
      "Epoch: 23 \tTraining Loss: 0.013537\n",
      "Epoch: 24 \tTraining Loss: 0.011889\n",
      "Epoch: 25 \tTraining Loss: 0.010464\n",
      "Epoch: 26 \tTraining Loss: 0.009082\n",
      "Epoch: 27 \tTraining Loss: 0.007881\n",
      "Epoch: 28 \tTraining Loss: 0.006778\n",
      "Epoch: 29 \tTraining Loss: 0.005897\n",
      "Epoch: 30 \tTraining Loss: 0.005112\n",
      "Epoch: 31 \tTraining Loss: 0.004456\n",
      "Epoch: 32 \tTraining Loss: 0.003942\n",
      "Epoch: 33 \tTraining Loss: 0.003488\n",
      "Epoch: 34 \tTraining Loss: 0.175600\n",
      "Epoch: 35 \tTraining Loss: 0.057492\n",
      "Epoch: 36 \tTraining Loss: 0.028664\n",
      "Epoch: 37 \tTraining Loss: 0.019852\n",
      "Epoch: 38 \tTraining Loss: 0.014625\n",
      "Epoch: 39 \tTraining Loss: 0.011159\n",
      "Epoch: 40 \tTraining Loss: 0.008681\n",
      "Epoch: 41 \tTraining Loss: 0.006836\n",
      "Epoch: 42 \tTraining Loss: 0.005521\n",
      "Epoch: 43 \tTraining Loss: 0.004559\n",
      "Epoch: 44 \tTraining Loss: 0.003841\n",
      "Epoch: 45 \tTraining Loss: 0.003312\n",
      "Epoch: 46 \tTraining Loss: 0.002918\n",
      "Epoch: 47 \tTraining Loss: 0.002636\n",
      "Epoch: 48 \tTraining Loss: 0.002412\n",
      "Epoch: 49 \tTraining Loss: 0.002229\n",
      "Epoch: 50 \tTraining Loss: 0.002085\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    bilstm_glove_model.train()\n",
    "    for sentences, is_capitals, lengths, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = bilstm_glove_model(sentences, is_capitals, lengths)\n",
    "        output = output.permute(0,2,1)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*sentences.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_dataset))\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cec3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bilstm_glove_model.state_dict(), 'bilstm_glove_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38d93c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilstm_glove_model = BiLSTMGlove(100, 256, 0.33, 128, 9)\n",
    "# bilstm_glove_model.load_state_dict(torch.load('bilstm_glove_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edfa950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevResultsGlove(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    f_read = open(\"./data/dev\",\"r\")\n",
    "    f_write = open(\"dev2.out\",\"w\")\n",
    "    for sentences, is_capitals, lengths, labels in dataloader:\n",
    "        output = model(sentences, is_capitals, lengths)\n",
    "        max_values, max_indices = torch.max(output, dim=2)\n",
    "        y = max_indices\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                read_line = f_read.readline().split()\n",
    "                if len(read_line)>0:\n",
    "                    f_write.write(str(read_line[0])+\" \"+str(read_line[1])+\" \"+index_label[labels[i][j].item()]+\" \"+index_label[y[i][j].item()]+\"\\n\")\n",
    "                else:\n",
    "                    break\n",
    "                if j+1>=len(sentences[i]):\n",
    "                    f_read.readline()\n",
    "            if len(sentences)==batch_size or i<len(sentences)-1:\n",
    "                f_write.write(\"\\n\")\n",
    "    f_read.close()\n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b113aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "getDevResultsGlove(bilstm_glove_model, dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a0363",
   "metadata": {},
   "source": [
    "#### The dev results are as follows:\n",
    "              \n",
    "              \n",
    "    processed 51578 tokens with 5942 phrases; found: 6097 phrases; correct: 5392.\n",
    "\n",
    "    accuracy:  98.25%; precision:  88.44%; recall:  90.74%; FB1:  89.58\n",
    "\n",
    "              LOC: precision:  94.00%; recall:  93.85%; FB1:  93.93  1834\n",
    "              \n",
    "             MISC: precision:  76.07%; recall:  84.49%; FB1:  80.06  1024\n",
    "             \n",
    "              ORG: precision:  82.55%; recall:  85.38%; FB1:  83.94  1387\n",
    "              \n",
    "              PER: precision:  94.17%; recall:  94.68%; FB1:  94.42  1852\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17a18b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestResultsGlove(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    f_read = open(\"./data/test\",\"r\")\n",
    "    f_write = open(\"test2.out\",\"w\")\n",
    "    for sentences, is_capitals, lengths in dataloader:\n",
    "        output = model(sentences, is_capitals, lengths)\n",
    "        max_values, max_indices = torch.max(output, dim=2)\n",
    "        y = max_indices\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                read_line = f_read.readline().split()\n",
    "                if len(read_line)>0:\n",
    "                    f_write.write(str(read_line[0])+\" \"+str(read_line[1])+\" \"+index_label[y[i][j].item()]+\"\\n\")\n",
    "                else:\n",
    "                    break\n",
    "                if j+1>=len(sentences[i]):\n",
    "                    f_read.readline()\n",
    "            if len(sentences)==batch_size or i<len(sentences)-1:\n",
    "                f_write.write(\"\\n\")\n",
    "    f_read.close()\n",
    "    f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9596804",
   "metadata": {},
   "outputs": [],
   "source": [
    "getTestResultsGlove(bilstm_glove_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
