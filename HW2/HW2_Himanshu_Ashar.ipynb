{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266651ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "import copy\n",
    "import json\n",
    "import toolz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8385b4",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcae46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/train\",\"r\")\n",
    "count_dict = defaultdict(int)\n",
    "for line in f:\n",
    "    get_words = line.split()\n",
    "    if len(get_words)!=0:\n",
    "        count_dict[get_words[1]]+=1\n",
    "f.close()\n",
    "\n",
    "unkw = 0\n",
    "for key,val in count_dict.items():\n",
    "    if val<2:\n",
    "        unkw += val\n",
    "\n",
    "sorted_count_list = sorted(count_dict.items(),key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cd3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/vocab.txt\", \"w\")\n",
    "f.write('<unk>\\t0\\t'+str(unkw)+'\\n')\n",
    "i=1\n",
    "vocab_count=0\n",
    "vocab_list = []\n",
    "for word,count in sorted_count_list:\n",
    "    if count>=2: \n",
    "        vocab_count += 1\n",
    "        vocab_list.append(word)\n",
    "        f.write(word+'\\t'+str(i)+'\\t'+str(count)+'\\n')\n",
    "        i+=1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423421c9",
   "metadata": {},
   "source": [
    "The selected threshold for unknown words is 2, i.e. word occuring less than 2 times are not included in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad70bb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size of the vocabulary is 23182.\n",
      "This is excluding the '<unk>' token.\n",
      "The total occurences of the special token '<unk>' after replacement is 20011.\n"
     ]
    }
   ],
   "source": [
    "print(\"The total size of the vocabulary is \"+str(vocab_count)+\".\")\n",
    "print(\"This is excluding the '<unk>' token.\")\n",
    "print(\"The total occurences of the special token '<unk>' after replacement is \"+str(unkw)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaedb97",
   "metadata": {},
   "source": [
    "### Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a654bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_counts = defaultdict(int)\n",
    "e_counts = defaultdict(int)\n",
    "t_counts = defaultdict(int)\n",
    "prev_s = \"start\"\n",
    "s_counts[\"start\"] += 1\n",
    "f = open(\"./data/train\", \"r\")\n",
    "for line in f:\n",
    "    get_indiv = line.split()\n",
    "    if(len(get_indiv)!=0):\n",
    "        t_counts[(prev_s,get_indiv[2])]+=1\n",
    "        if get_indiv[1] in vocab_list:\n",
    "            e_counts[(get_indiv[2],get_indiv[1])]+=1\n",
    "        else:\n",
    "            e_counts[(get_indiv[2],'<unk>')]+=1\n",
    "        s_counts[get_indiv[2]]+=1\n",
    "        prev_s = get_indiv[2]\n",
    "    else:\n",
    "        prev_s=\"start\"\n",
    "        s_counts[\"start\"] += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99465b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = defaultdict(int)\n",
    "for key,val in t_counts.items():\n",
    "    transition[key] = t_counts[key]/s_counts[key[0]]\n",
    "    \n",
    "emission = defaultdict(int)\n",
    "for key,val in e_counts.items():\n",
    "    emission[key] = e_counts[key]/s_counts[key[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215320e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of transition parameters in HMM: 1392\n",
      "The number of emission parameters in HMM: 30303\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of transition parameters in HMM:\",str(len(transition.keys())))\n",
    "print(\"The number of emission parameters in HMM:\",str(len(emission.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d5ed1",
   "metadata": {},
   "source": [
    "#### Note: The total number of transition parameters would usually be 45x46 = 2070, and the total number of emission parameters would usually be 45x23182 = 1043190 , where 23182 is (size of vocab). However, here this number is less, because only those combinations which exist in our training data are calculated.\n",
    "\n",
    "#### Getting each other possible combination and assigning it a probability of 0 is not necessary as pointed out by one of the TAs, hence we do not compute all such cases, and only report the values for which we got transition and emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ffb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = copy.deepcopy(list(s_counts.keys()))\n",
    "tags.remove('start') #this is done because start is not an actual tag. It was only taken in s counts to help compute probabilities when a word was at the start of the sentence - prior prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba747bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tup_to_str(x):\n",
    "    return str(x)\n",
    "\n",
    "transition_json = copy.deepcopy(transition)\n",
    "emission_json = copy.deepcopy(emission)\n",
    "transition_json = toolz.keymap(tup_to_str, transition_json)\n",
    "emission_json = toolz.keymap(tup_to_str, emission_json)\n",
    "total_dict = {'transition':transition_json, 'emission':emission_json}\n",
    "with open(\"./data/hmm.json\",\"w\") as output_file:\n",
    "    json.dump(total_dict, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc1e2c",
   "metadata": {},
   "source": [
    "### Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0daaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyDecoding(data):\n",
    "    if data==\"dev\":\n",
    "        actual_tags = []\n",
    "        predicted_tags = []\n",
    "        prev_tag=\"start\" #since the first line of file starts with a sentence, we need to mention \"start\" so prior probability is computed.\n",
    "        f = open(\"./data/dev\", \"r\")\n",
    "        for line in f:\n",
    "            get_indiv = line.split()\n",
    "            if len(get_indiv)>0:\n",
    "                actual_tags.append(get_indiv[2])\n",
    "                max_pred_tag = [-1,None]\n",
    "                for state in tags:\n",
    "                    if get_indiv[1] in vocab_list:\n",
    "                        em_prob = emission[(state,get_indiv[1])]\n",
    "                    else:\n",
    "                        em_prob = emission[(state,'<unk>')]\n",
    "                    trans_prob = transition[(prev_tag,state)]\n",
    "                    prob = em_prob*trans_prob\n",
    "                    if prob > max_pred_tag[0]:\n",
    "                        max_pred_tag = [prob, state]\n",
    "                prev_tag = max_pred_tag[1]\n",
    "                predicted_tags.append(max_pred_tag[1])\n",
    "            else:\n",
    "                prev_tag = \"start\"\n",
    "        f.close()\n",
    "        return actual_tags, predicted_tags\n",
    "    elif data==\"test\":\n",
    "        #predicted_tags = []\n",
    "        prev_tag=\"start\" #since the first line of file starts with a sentence, we need to mention \"start\" so prior probability is computed.\n",
    "        f_test = open(\"./data/test\", \"r\")\n",
    "        model_out = open(\"./data/greedy.out\",\"w\")\n",
    "        i=1\n",
    "        for line in f_test:\n",
    "            get_indiv = line.split()\n",
    "            if len(get_indiv)>0:\n",
    "                max_pred_tag = [-1,None]\n",
    "                for state in tags:\n",
    "                    if get_indiv[1] in vocab_list:\n",
    "                        em_prob = emission[(state,get_indiv[1])]\n",
    "                    else:\n",
    "                        em_prob = emission[(state,'<unk>')]\n",
    "                    trans_prob = transition[(prev_tag,state)]\n",
    "                    prob = em_prob*trans_prob\n",
    "                    if prob > max_pred_tag[0]:\n",
    "                        max_pred_tag = [prob, state]\n",
    "                prev_tag = max_pred_tag[1]\n",
    "                #predicted_tags.append(max_pred_tag[1])\n",
    "                model_out.write(str(i)+\"\\t\"+get_indiv[1]+\"\\t\"+max_pred_tag[1]+\"\\n\")\n",
    "                i+=1\n",
    "            else:\n",
    "                prev_tag = \"start\"\n",
    "                model_out.write(\"\\n\")\n",
    "                i=1\n",
    "        f_test.close()\n",
    "        model_out.close()\n",
    "        print(\"greedy.out file created in data folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821d67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevAccuracy(actual_tags, predicted_tags):\n",
    "    true_pred = 0\n",
    "    for i in range(len(actual_tags)):\n",
    "        if actual_tags[i]==predicted_tags[i]:\n",
    "            true_pred+=1\n",
    "    return true_pred/len(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec404218",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_tags, predicted_tags = greedyDecoding('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08de5055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Greedy Decoding on dev data: 0.9350297492562686\n"
     ]
    }
   ],
   "source": [
    "dev_accuracy = getDevAccuracy(actual_tags,predicted_tags)\n",
    "print(\"Accuracy of Greedy Decoding on dev data:\",dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b8d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy.out file created in data folder.\n"
     ]
    }
   ],
   "source": [
    "greedyDecoding('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cb697",
   "metadata": {},
   "source": [
    "### Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a54080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbiDecoding(data):\n",
    "    if data==\"dev\":\n",
    "        f = open('./data/dev', 'r')\n",
    "        actual_tags = []\n",
    "        predicted_tags = []\n",
    "        prev_tag=\"start\" #since the first line of file starts with a sentence, we need to mention \"start\" so prior probability is computed.\n",
    "        for line in f:\n",
    "            get_indiv = line.split()\n",
    "            if len(get_indiv)>0:\n",
    "                actual_tags.append(get_indiv[2])\n",
    "                if prev_tag==\"start\":\n",
    "                    viterbi=[]\n",
    "                    first_dict = {}\n",
    "                    for state in tags:\n",
    "                        if get_indiv[1] in vocab_list:\n",
    "                            em_prob = emission[(state,get_indiv[1])]\n",
    "                        else:\n",
    "                            em_prob = emission[(state,'<unk>')]\n",
    "                        trans_prob = transition[(prev_tag,state)]\n",
    "                        prob = em_prob*trans_prob\n",
    "                        first_dict[state] = (prob, prev_tag)\n",
    "                    viterbi.append(copy.deepcopy(first_dict))\n",
    "                    prev_tag=\"not start\"\n",
    "                else:\n",
    "                    curr_dict = {}\n",
    "                    for state in tags:\n",
    "                        if get_indiv[1] in vocab_list:\n",
    "                            em_prob = emission[(state,get_indiv[1])]\n",
    "                        else:\n",
    "                            em_prob = emission[(state,'<unk>')]\n",
    "                        max_state_prob = [-1,None]\n",
    "                        for prev_state_key, prev_state_val in viterbi[-1].items():\n",
    "                            trans_prob = transition[(prev_state_key,state)]\n",
    "                            prev_state_prob_val = prev_state_val[0]\n",
    "                            final_prob = em_prob*trans_prob*prev_state_prob_val\n",
    "                            if final_prob>max_state_prob[0]:\n",
    "                                max_state_prob = [final_prob, prev_state_key]\n",
    "                        curr_dict[state] = (max_state_prob[0], max_state_prob[1])\n",
    "                    viterbi.append(copy.deepcopy(curr_dict))\n",
    "            else:\n",
    "                preds = []\n",
    "                max_val = max(viterbi[len(viterbi)-1].values(), key = lambda x: x[0])\n",
    "                preds.append(next(key for key,val in viterbi[len(viterbi)-1].items() if val==max_val))\n",
    "                prev_state=max_val[1]\n",
    "                for i in range(len(viterbi)-2, -1, -1):\n",
    "                    preds.append(prev_state)\n",
    "                    prev_state = viterbi[i][prev_state][1]\n",
    "                preds.reverse()\n",
    "                predicted_tags.extend(preds)\n",
    "                prev_tag = \"start\"\n",
    "        f.close()\n",
    "        preds = []\n",
    "        max_val = max(viterbi[len(viterbi)-1].values(), key = lambda x: x[0])\n",
    "        preds.append(next(key for key,val in viterbi[len(viterbi)-1].items() if val==max_val))\n",
    "        prev_state=max_val[1]\n",
    "        for i in range(len(viterbi)-2, -1, -1):\n",
    "            preds.append(prev_state)\n",
    "            prev_state = viterbi[i][prev_state][1]\n",
    "        preds.reverse()\n",
    "        predicted_tags.extend(preds)\n",
    "        \n",
    "    elif data==\"test\":\n",
    "        f = open('./data/test', 'r')\n",
    "        predicted_tags = []\n",
    "        prev_tag=\"start\" #since the first line of file starts with a sentence, we need to mention \"start\" so prior probability is computed.\n",
    "        for line in f:\n",
    "            get_indiv = line.split()\n",
    "            if len(get_indiv)>0:\n",
    "                if prev_tag==\"start\":\n",
    "                    viterbi=[]\n",
    "                    first_dict = {}\n",
    "                    for state in tags:\n",
    "                        if get_indiv[1] in vocab_list:\n",
    "                            em_prob = emission[(state,get_indiv[1])]\n",
    "                        else:\n",
    "                            em_prob = emission[(state,'<unk>')]\n",
    "                        trans_prob = transition[(prev_tag,state)]\n",
    "                        prob = em_prob*trans_prob\n",
    "                        first_dict[state] = (prob, prev_tag)\n",
    "                    viterbi.append(copy.deepcopy(first_dict))\n",
    "                    prev_tag=\"not start\"\n",
    "                else:\n",
    "                    curr_dict = {}\n",
    "                    for state in tags:\n",
    "                        if get_indiv[1] in vocab_list:\n",
    "                            em_prob = emission[(state,get_indiv[1])]\n",
    "                        else:\n",
    "                            em_prob = emission[(state,'<unk>')]\n",
    "                        max_state_prob = [-1,None]\n",
    "                        for prev_state_key, prev_state_val in viterbi[-1].items():\n",
    "                            trans_prob = transition[(prev_state_key,state)]\n",
    "                            prev_state_prob_val = prev_state_val[0]\n",
    "                            final_prob = em_prob*trans_prob*prev_state_prob_val\n",
    "                            if final_prob>max_state_prob[0]:\n",
    "                                max_state_prob = [final_prob, prev_state_key]\n",
    "                        curr_dict[state] = (max_state_prob[0], max_state_prob[1])\n",
    "                    viterbi.append(copy.deepcopy(curr_dict))\n",
    "            else:\n",
    "                preds = []\n",
    "                max_val = max(viterbi[len(viterbi)-1].values(), key = lambda x: x[0])\n",
    "                preds.append(next(key for key,val in viterbi[len(viterbi)-1].items() if val==max_val))\n",
    "                prev_state=max_val[1]\n",
    "                for i in range(len(viterbi)-2, -1, -1):\n",
    "                    preds.append(prev_state)\n",
    "                    prev_state = viterbi[i][prev_state][1]\n",
    "                preds.reverse()\n",
    "                predicted_tags.extend(preds)\n",
    "                prev_tag = \"start\"\n",
    "        f.close()\n",
    "        preds = []\n",
    "        max_val = max(viterbi[len(viterbi)-1].values(), key = lambda x: x[0])\n",
    "        preds.append(next(key for key,val in viterbi[len(viterbi)-1].items() if val==max_val))\n",
    "        prev_state=max_val[1]\n",
    "        for i in range(len(viterbi)-2, -1, -1):\n",
    "            preds.append(prev_state)\n",
    "            prev_state = viterbi[i][prev_state][1]\n",
    "        preds.reverse()\n",
    "        predicted_tags.extend(preds)\n",
    "        \n",
    "    if data==\"dev\":\n",
    "        return actual_tags, predicted_tags\n",
    "    elif data==\"test\":\n",
    "        model_out = open('./data/viterbi.out','w')\n",
    "        f = open('./data/test','r')\n",
    "        i=0\n",
    "        for line in f:\n",
    "            get_indiv = line.split()\n",
    "            if len(get_indiv)>0:\n",
    "                model_out.write(str(get_indiv[0])+\"\\t\"+get_indiv[1]+\"\\t\"+predicted_tags[i]+\"\\n\")\n",
    "                i+=1\n",
    "            else:\n",
    "                model_out.write(\"\\n\")\n",
    "        f.close()\n",
    "        model_out.close()\n",
    "        print(\"viterbi.out file created in data folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752d211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_tags, predicted_tags = viterbiDecoding('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae92093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Viterbi Decoding on dev data: 0.9476883613623945\n"
     ]
    }
   ],
   "source": [
    "dev_accuracy = getDevAccuracy(actual_tags, predicted_tags)\n",
    "print(\"Accuracy of Viterbi Decoding on dev data:\",dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4af1ddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi.out file created in data folder.\n"
     ]
    }
   ],
   "source": [
    "viterbiDecoding('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
