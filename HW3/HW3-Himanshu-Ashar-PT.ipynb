{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdad31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshuashar/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim.downloader as w2vapi\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf66b3",
   "metadata": {},
   "source": [
    "#### Package versions:\n",
    "\n",
    "Python: 3.9.7\n",
    "\n",
    "Gensim: 4.3.0\n",
    "\n",
    "PyTorch: 1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dea06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a865c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43782d0d",
   "metadata": {},
   "source": [
    "#### Note: I have not included the data.tsv file in my submission as it was stated in the HW description that it is not required.\n",
    "\n",
    "However, I have included these 6 extra files:\n",
    "\n",
    "custom_word2vec.model, fnn_model_avg.pt, fnn_model_concat.pt, rnn_model.pt, gru_model.pt, lstm_model.pt.\n",
    "\n",
    "The .pt files are the best outcomes of my models based on validation loss. If needed, they can be deleted before execution, as this notebook will automatically generate them again when executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326944a8",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04933f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.tsv',on_bad_lines='skip',sep='\\t')\n",
    "data = data[['review_body','star_rating']]\n",
    "data = data.dropna()\n",
    "\n",
    "data['rating_class'] = 0\n",
    "data.loc[(data['star_rating']==1) | (data['star_rating']=='1') | (data['star_rating']==2) | (data['star_rating']=='2'),'rating_class'] = 0\n",
    "data.loc[(data['star_rating']==3) | (data['star_rating']=='3'),'rating_class'] = 1\n",
    "data.loc[(data['star_rating']==4) | (data['star_rating']=='4') | (data['star_rating']==5) | (data['star_rating']=='5'),'rating_class'] = 2\n",
    "\n",
    "class_1_20k = data.loc[data['rating_class']==0].sample(n=20000, random_state=47)\n",
    "class_2_20k = data.loc[data['rating_class']==1].sample(n=20000, random_state=47)\n",
    "class_3_20k = data.loc[data['rating_class']==2].sample(n=20000, random_state=47) #47\n",
    "\n",
    "df = pd.concat([class_1_20k, class_2_20k, class_3_20k], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd676ae",
   "metadata": {},
   "source": [
    "The above creates a dataset of 60,000 reviews, with 20,000 reviews of each created category:\n",
    "\n",
    "Rating class 0: rating 1, rating 2\n",
    "\n",
    "Rating class 1: rating 3\n",
    "\n",
    "Rating class 2: rating 4, rating 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01418bca",
   "metadata": {},
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c477351",
   "metadata": {},
   "source": [
    "### (a) Load pre-trained word embeddings and experiment with some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd19a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = w2vapi.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fba8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2vapi.load('word2vec-google-news-300',return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b2a86e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65639\n"
     ]
    }
   ],
   "source": [
    "print(w2v.similarity('important','essential'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56bc5095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22030458\n"
     ]
    }
   ],
   "source": [
    "print(w2v.similarity('return','replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced7ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73467165\n"
     ]
    }
   ],
   "source": [
    "print(w2v.similarity('expensive','costly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9624b642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25702554\n"
     ]
    }
   ],
   "source": [
    "print(w2v.similarity('product','item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b4ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4600251\n"
     ]
    }
   ],
   "source": [
    "print(w2v.similarity('service','customer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "308a4e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053421497\n"
     ]
    }
   ],
   "source": [
    "print(w2v.similarity('satisfied','pleased') - w2v.similarity('dissatisfied','disappointed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cef400",
   "metadata": {},
   "source": [
    "The above 6 examples are meant to show semantic similarity between words, using the pretrained Google word embeddings.\n",
    "\n",
    "Examples chosen are of words which are likely to appear in customer reviews, to make a comprehensive comparison with the model trained from our own dataset, in 2(b) below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830fb19",
   "metadata": {},
   "source": [
    "### (b) Train a Word2Vec model using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a710cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsCorpus:\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for index, row in df.iterrows():\n",
    "            row_sents = str(row['review_body'])\n",
    "            row_sents = row_sents.replace(',','')\n",
    "            row_sents = row_sents.split('.')\n",
    "            for sent in row_sents:\n",
    "                yield utils.simple_preprocess(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5500e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ReviewsCorpus()\n",
    "w2v_custom_model = gensim.models.Word2Vec(sentences=sentences, vector_size=300, window=13, min_count=9)\n",
    "w2v_custom_model.save('custom_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cd4ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18706413\n"
     ]
    }
   ],
   "source": [
    "print(w2v_custom_model.wv.similarity('important','essential'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62e84ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5946355\n"
     ]
    }
   ],
   "source": [
    "print(w2v_custom_model.wv.similarity('return','replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df97210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6565095\n"
     ]
    }
   ],
   "source": [
    "print(w2v_custom_model.wv.similarity('expensive','costly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e8c7463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7072196\n"
     ]
    }
   ],
   "source": [
    "print(w2v_custom_model.wv.similarity('product','item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a352c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8479623\n"
     ]
    }
   ],
   "source": [
    "print(w2v_custom_model.wv.similarity('service','customer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7cd9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27745116\n"
     ]
    }
   ],
   "source": [
    "print(w2v_custom_model.wv.similarity('satisfied','pleased') - w2v_custom_model.wv.similarity('dissatisfied','disappointed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc2be0",
   "metadata": {},
   "source": [
    "#### As seen above, the custom model accurately depicts similarity between words whose context is more similar in our dataset, compared to Word2Vec pretrained embeddings.\n",
    "\n",
    "(Return, replace), (product, item), (service, customer) have greater relation between each other when we consider our dataset of reviews, hence their semantic similarity is higher.\n",
    "\n",
    "(Important, essential), (expensive, costly) are not strictly related to our dataset, hence their similarities are better computed from the pre-trained embeddings, which are trained over a much larger corpus.\n",
    "\n",
    "Also, the similarity between pleased and satisified, and between disappointed and dissatisfied is shown better in manual trained compared to pre-trained. In short Pleased - Disappointed + Satisifed = Dissatisifed is shown better in manual trained. This is because the above words are again more likely to appear in customer reviews.\n",
    "\n",
    "To conclude, the custom model accurately depicts semantic similarity between words whose context is more similar in our dataset, compared to word2vec pretrained embeddings. But general words' semantic similarity is better in the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332a9f1",
   "metadata": {},
   "source": [
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c93d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_avg = []\n",
    "for i in range(df.shape[0]):\n",
    "    curr_review = df.iloc[i]['review_body']\n",
    "    curr_review = curr_review.replace(',','') #removing commas and periods is necessary because otherwise words will not be recognized by the Word2Vec model\n",
    "    curr_review = curr_review.replace('.','')\n",
    "    curr_review = curr_review.split()\n",
    "    curr_vect = []\n",
    "    for word in curr_review:\n",
    "        if word in w2v:\n",
    "            curr_vect.append(w2v[word])\n",
    "    if len(curr_vect)==0:\n",
    "        curr_vect = np.zeros((300,), dtype=float)\n",
    "#         curr_vect = np.array(curr_vect)\n",
    "    else:\n",
    "        curr_vect = np.array(curr_vect)\n",
    "        curr_vect = np.mean(curr_vect,axis=0)\n",
    "    X_avg.append(curr_vect)\n",
    "X_avg = np.array(X_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e8521",
   "metadata": {},
   "source": [
    "Note: Even though no data pre-processing or cleaning was conducted on data, full stops and commas were removed.\n",
    "\n",
    "This was necessary because the Word2Vec model would not be able to recognize words if they are attached behind a full stop or comma. Eg. Word embedding would not exist for 'happy,' or 'happy.', but will exist for 'happy'.\n",
    "\n",
    "Hence, to pass only the word to the Word2Vec model, the commas and full stops are removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a1a8315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1359a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_avg, df['rating_class'], test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce703eb5",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f26cdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "percep = Perceptron(penalty='elasticnet',alpha=0.00001, random_state=168)\n",
    "percep = percep.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fddc81cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when using Word2Vec features: 0.5851666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy when using Word2Vec features:',str(percep.score(X_test,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b940a",
   "metadata": {},
   "source": [
    "Accuracy when using TF-IDF features: 0.6384166666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6eabd4",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b6cb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_svc = LinearSVC(penalty='l1', dual=False,C=0.3)\n",
    "lin_svc = lin_svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5934c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when using Word2Vec features: 0.6538333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy when using Word2Vec features:',str(lin_svc.score(X_test,Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0139aa",
   "metadata": {},
   "source": [
    "Accuracy when using TF-IDF features: 0.7143333333333334"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78970268",
   "metadata": {},
   "source": [
    "#### It is seen that the models performed better when using TF-IDF features, compared to Word2Vec features.\n",
    "This can be because TF-IDF vectorizer would consider only words from our corpus of reviews, while the Word2Vec pre-trained embeddings are trained on a large corpus, which contains general words of all kinds.\n",
    "\n",
    "Some words in reviews tend to be common, and TF-IDF enables the functionality which shows how important a word is in a document based on its frequency in a corpus. This may have led to a better performance than Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7ca6d",
   "metadata": {},
   "source": [
    "#### Note: The accuracies for both of the above models when computed using TF-IDF features is a part of HW1. The random seeds for dataset generation, as well as model hyperparameters are the same. The only difference is that HW1 used TF-IDF features. I have only reported those numbers here, as was told by Professor during one of the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59618284",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07e52d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(pd.DataFrame(df['review_body']), pd.DataFrame(df['rating_class']), test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f98abfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader):\n",
    "    prediction_list = []\n",
    "    actual_list = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        outputs = model(batch[0])\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        prediction_list.append(int(predicted[0]))\n",
    "        actual_list.append(int(batch[1][0]))\n",
    "    \n",
    "    total=0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i]==actual_list[i]:\n",
    "            total+=1\n",
    "    return float(total)/len(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b4dc1",
   "metadata": {},
   "source": [
    "### (a) Taking the average of all Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08065c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9517fc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a153e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataAvg:\n",
    "    \n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        curr_review = self.reviews.iloc[i]['review_body']\n",
    "        curr_review = curr_review.replace(',','')\n",
    "        curr_review = curr_review.replace('.','')\n",
    "        curr_review = curr_review.split()\n",
    "        curr_vect = []\n",
    "        for word in curr_review:\n",
    "            if word in w2v:\n",
    "                curr_vect.append(w2v[word])\n",
    "        if len(curr_vect)==0:\n",
    "            curr_vect = np.zeros((300,), dtype=np.float32)\n",
    "    #         curr_vect = np.array(curr_vect)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "            curr_vect = np.mean(curr_vect,axis=0)\n",
    "        \n",
    "        curr_vect = torch.from_numpy(curr_vect)\n",
    "        \n",
    "        label = self.ratings.iloc[i]['rating_class']\n",
    "        \n",
    "        return curr_vect, label\n",
    "    \n",
    "\n",
    "\n",
    "class TestDataAvg:\n",
    "    \n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        curr_review = self.reviews.iloc[i]['review_body']\n",
    "        curr_review = curr_review.replace(',','')\n",
    "        curr_review = curr_review.replace('.','')\n",
    "        curr_review = curr_review.split()\n",
    "        curr_vect = []\n",
    "        for word in curr_review:\n",
    "            if word in w2v:\n",
    "                curr_vect.append(w2v[word])\n",
    "        if len(curr_vect)==0:\n",
    "            curr_vect = np.zeros((300,), dtype=np.float32)\n",
    "    #         curr_vect = np.array(curr_vect)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "            curr_vect = np.mean(curr_vect,axis=0)\n",
    "        \n",
    "        curr_vect = torch.from_numpy(curr_vect)\n",
    "        \n",
    "        label = self.ratings.iloc[i]['rating_class']\n",
    "        \n",
    "        return curr_vect, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9548cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_avg = TrainDataAvg(X_train,Y_train)\n",
    "test_data_avg = TestDataAvg(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8c59528",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "validation_size=0.2\n",
    "\n",
    "num_train = len(train_data_avg)\n",
    "inds = list(range(num_train))\n",
    "np.random.shuffle(inds)\n",
    "split = int(np.floor(validation_size*num_train))\n",
    "train_idx, valid_idx = inds[split:], inds[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validn_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_data_avg, batch_size=batch_size, sampler=train_sampler)\n",
    "validn_loader = DataLoader(train_data_avg, batch_size=batch_size, sampler=validn_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1c23e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNetAvg(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FFNetAvg(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FFNetAvg, self).__init__()\n",
    "        \n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "fnn_model_avg = FFNetAvg()\n",
    "print(fnn_model_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b498589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fnn_model_avg.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5088a82a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.981316 \tValidation Loss: 0.891588\n",
      "Validation loss decreased (inf --> 0.891588).  Saving model.\n",
      "Epoch: 2 \tTraining Loss: 0.875733 \tValidation Loss: 0.848071\n",
      "Validation loss decreased (0.891588 --> 0.848071).  Saving model.\n",
      "Epoch: 3 \tTraining Loss: 0.844206 \tValidation Loss: 0.829830\n",
      "Validation loss decreased (0.848071 --> 0.829830).  Saving model.\n",
      "Epoch: 4 \tTraining Loss: 0.829030 \tValidation Loss: 0.816238\n",
      "Validation loss decreased (0.829830 --> 0.816238).  Saving model.\n",
      "Epoch: 5 \tTraining Loss: 0.819646 \tValidation Loss: 0.811278\n",
      "Validation loss decreased (0.816238 --> 0.811278).  Saving model.\n",
      "Epoch: 6 \tTraining Loss: 0.810481 \tValidation Loss: 0.806048\n",
      "Validation loss decreased (0.811278 --> 0.806048).  Saving model.\n",
      "Epoch: 7 \tTraining Loss: 0.806280 \tValidation Loss: 0.803247\n",
      "Validation loss decreased (0.806048 --> 0.803247).  Saving model.\n",
      "Epoch: 8 \tTraining Loss: 0.801907 \tValidation Loss: 0.801237\n",
      "Validation loss decreased (0.803247 --> 0.801237).  Saving model.\n",
      "Epoch: 9 \tTraining Loss: 0.797223 \tValidation Loss: 0.799321\n",
      "Validation loss decreased (0.801237 --> 0.799321).  Saving model.\n",
      "Epoch: 10 \tTraining Loss: 0.793840 \tValidation Loss: 0.795852\n",
      "Validation loss decreased (0.799321 --> 0.795852).  Saving model.\n",
      "Epoch: 11 \tTraining Loss: 0.790485 \tValidation Loss: 0.795029\n",
      "Validation loss decreased (0.795852 --> 0.795029).  Saving model.\n",
      "Epoch: 12 \tTraining Loss: 0.785786 \tValidation Loss: 0.794765\n",
      "Validation loss decreased (0.795029 --> 0.794765).  Saving model.\n",
      "Epoch: 13 \tTraining Loss: 0.784035 \tValidation Loss: 0.793330\n",
      "Validation loss decreased (0.794765 --> 0.793330).  Saving model.\n",
      "Epoch: 14 \tTraining Loss: 0.779533 \tValidation Loss: 0.795258\n",
      "Epoch: 15 \tTraining Loss: 0.778603 \tValidation Loss: 0.796288\n",
      "Epoch: 16 \tTraining Loss: 0.775730 \tValidation Loss: 0.789369\n",
      "Validation loss decreased (0.793330 --> 0.789369).  Saving model.\n",
      "Epoch: 17 \tTraining Loss: 0.776627 \tValidation Loss: 0.790748\n",
      "Epoch: 18 \tTraining Loss: 0.768383 \tValidation Loss: 0.789664\n",
      "Epoch: 19 \tTraining Loss: 0.765278 \tValidation Loss: 0.798539\n",
      "Epoch: 20 \tTraining Loss: 0.766675 \tValidation Loss: 0.784882\n",
      "Validation loss decreased (0.789369 --> 0.784882).  Saving model.\n",
      "Epoch: 21 \tTraining Loss: 0.760929 \tValidation Loss: 0.789845\n",
      "Epoch: 22 \tTraining Loss: 0.758705 \tValidation Loss: 0.784307\n",
      "Validation loss decreased (0.784882 --> 0.784307).  Saving model.\n",
      "Epoch: 23 \tTraining Loss: 0.758874 \tValidation Loss: 0.787150\n",
      "Epoch: 24 \tTraining Loss: 0.754709 \tValidation Loss: 0.782804\n",
      "Validation loss decreased (0.784307 --> 0.782804).  Saving model.\n",
      "Epoch: 25 \tTraining Loss: 0.749129 \tValidation Loss: 0.781829\n",
      "Validation loss decreased (0.782804 --> 0.781829).  Saving model.\n",
      "Epoch: 26 \tTraining Loss: 0.750195 \tValidation Loss: 0.780964\n",
      "Validation loss decreased (0.781829 --> 0.780964).  Saving model.\n",
      "Epoch: 27 \tTraining Loss: 0.747532 \tValidation Loss: 0.779949\n",
      "Validation loss decreased (0.780964 --> 0.779949).  Saving model.\n",
      "Epoch: 28 \tTraining Loss: 0.744802 \tValidation Loss: 0.780286\n",
      "Epoch: 29 \tTraining Loss: 0.743132 \tValidation Loss: 0.781798\n",
      "Epoch: 30 \tTraining Loss: 0.740513 \tValidation Loss: 0.779513\n",
      "Validation loss decreased (0.779949 --> 0.779513).  Saving model.\n",
      "Epoch: 31 \tTraining Loss: 0.737226 \tValidation Loss: 0.778139\n",
      "Validation loss decreased (0.779513 --> 0.778139).  Saving model.\n",
      "Epoch: 32 \tTraining Loss: 0.734350 \tValidation Loss: 0.776672\n",
      "Validation loss decreased (0.778139 --> 0.776672).  Saving model.\n",
      "Epoch: 33 \tTraining Loss: 0.734621 \tValidation Loss: 0.776970\n",
      "Epoch: 34 \tTraining Loss: 0.732101 \tValidation Loss: 0.776635\n",
      "Validation loss decreased (0.776672 --> 0.776635).  Saving model.\n",
      "Epoch: 35 \tTraining Loss: 0.729548 \tValidation Loss: 0.773622\n",
      "Validation loss decreased (0.776635 --> 0.773622).  Saving model.\n",
      "Epoch: 36 \tTraining Loss: 0.727841 \tValidation Loss: 0.782525\n",
      "Epoch: 37 \tTraining Loss: 0.725484 \tValidation Loss: 0.782571\n",
      "Epoch: 38 \tTraining Loss: 0.722268 \tValidation Loss: 0.774031\n",
      "Epoch: 39 \tTraining Loss: 0.723487 \tValidation Loss: 0.774632\n",
      "Epoch: 40 \tTraining Loss: 0.717225 \tValidation Loss: 0.779714\n",
      "Epoch: 41 \tTraining Loss: 0.716115 \tValidation Loss: 0.783029\n",
      "Epoch: 42 \tTraining Loss: 0.717091 \tValidation Loss: 0.775708\n",
      "Epoch: 43 \tTraining Loss: 0.713812 \tValidation Loss: 0.774633\n",
      "Epoch: 44 \tTraining Loss: 0.711862 \tValidation Loss: 0.775239\n",
      "Epoch: 45 \tTraining Loss: 0.709528 \tValidation Loss: 0.775167\n",
      "Epoch: 46 \tTraining Loss: 0.706827 \tValidation Loss: 0.771833\n",
      "Validation loss decreased (0.773622 --> 0.771833).  Saving model.\n",
      "Epoch: 47 \tTraining Loss: 0.706073 \tValidation Loss: 0.772032\n",
      "Epoch: 48 \tTraining Loss: 0.701980 \tValidation Loss: 0.776347\n",
      "Epoch: 49 \tTraining Loss: 0.702528 \tValidation Loss: 0.768803\n",
      "Validation loss decreased (0.771833 --> 0.768803).  Saving model.\n",
      "Epoch: 50 \tTraining Loss: 0.699484 \tValidation Loss: 0.774433\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "validn_min_loss = np.Inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    validn_loss = 0.0\n",
    "    \n",
    "    fnn_model_avg.train()\n",
    "    for rev, rev_class in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = fnn_model_avg(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*rev.size(0)\n",
    "        \n",
    "    fnn_model_avg.eval()\n",
    "    for rev, rev_class in validn_loader:\n",
    "        output = fnn_model_avg(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        \n",
    "        validn_loss += loss.item()*rev.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_loader)*batch_size)\n",
    "    validn_loss = validn_loss/(len(validn_loader)*batch_size)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, validn_loss))\n",
    "    \n",
    "    if validn_loss <= validn_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model.'.format(validn_min_loss, validn_loss))\n",
    "        torch.save(fnn_model_avg.state_dict(), 'fnn_model_avg.pt')\n",
    "        validn_min_loss = validn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fd52ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data_avg, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "259c11c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_model_avg = FFNetAvg()\n",
    "fnn_model_avg.load_state_dict(torch.load('fnn_model_avg.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ec0b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FNN using average Word2Vec vectors: 0.6505\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of FNN using average Word2Vec vectors:',str(accuracy(fnn_model_avg, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fe092",
   "metadata": {},
   "source": [
    "### (b) Concatenating the first 10 Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67cadce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataConcat:\n",
    "    \n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        curr_review = self.reviews.iloc[i]['review_body']\n",
    "        curr_review = curr_review.replace(',','')\n",
    "        curr_review = curr_review.replace('.','')\n",
    "        curr_review = curr_review.split()\n",
    "        curr_vect = []\n",
    "        word_count = 0\n",
    "        for word in curr_review:\n",
    "            if word_count==10:\n",
    "                break\n",
    "            if word in w2v:\n",
    "                word_count+=1\n",
    "                curr_vect.append(w2v[word])\n",
    "\n",
    "        while word_count<10:\n",
    "            curr_vect.append(np.zeros((300,), dtype=np.float32))\n",
    "            word_count+=1\n",
    "\n",
    "        if len(curr_vect)==0:\n",
    "            curr_vect = np.zeros((3000,), dtype=np.float32)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "            curr_vect = curr_vect.flatten()\n",
    "\n",
    "        curr_vect = torch.from_numpy(curr_vect)\n",
    "        \n",
    "        label = self.ratings.iloc[i]['rating_class']\n",
    "        \n",
    "        return curr_vect, label\n",
    "    \n",
    "\n",
    "\n",
    "class TestDataConcat:\n",
    "    \n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        curr_review = self.reviews.iloc[i]['review_body']\n",
    "        curr_review = curr_review.replace(',','')\n",
    "        curr_review = curr_review.replace('.','')\n",
    "        curr_review = curr_review.split()\n",
    "        curr_vect = []\n",
    "        word_count = 0\n",
    "        for word in curr_review:\n",
    "            if word_count==10:\n",
    "                break\n",
    "            if word in w2v:\n",
    "                word_count+=1\n",
    "                curr_vect.append(w2v[word])\n",
    "\n",
    "        while word_count<10:\n",
    "            curr_vect.append(np.zeros((300,),dtype=np.float32))\n",
    "            word_count+=1\n",
    "\n",
    "        if len(curr_vect)==0:\n",
    "            curr_vect = np.zeros((3000,),dtype=np.float32)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "            curr_vect = curr_vect.flatten()\n",
    "\n",
    "        curr_vect = torch.from_numpy(curr_vect)\n",
    "        \n",
    "        label = self.ratings.iloc[i]['rating_class']\n",
    "        \n",
    "        return curr_vect, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8920ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_concat = TrainDataConcat(X_train,Y_train)\n",
    "test_data_concat = TestDataConcat(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3e56a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "validation_size=0.2\n",
    "\n",
    "num_train = len(train_data_concat)\n",
    "inds = list(range(num_train))\n",
    "np.random.shuffle(inds)\n",
    "split = int(np.floor(validation_size*num_train))\n",
    "train_idx, valid_idx = inds[split:], inds[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validn_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_data_concat, batch_size=batch_size, sampler=train_sampler)\n",
    "validn_loader = DataLoader(train_data_concat, batch_size=batch_size, sampler=validn_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7515987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNetConcat(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FFNetConcat(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FFNetConcat, self).__init__()\n",
    "        \n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "#         x = x.to(torch.float32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "fnn_model_concat = FFNetConcat()\n",
    "print(fnn_model_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6893106",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fnn_model_concat.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea99000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.094918 \tValidation Loss: 1.073728\n",
      "Validation loss decreased (inf --> 1.073728).  Saving model.\n",
      "Epoch: 2 \tTraining Loss: 1.051941 \tValidation Loss: 1.025497\n",
      "Validation loss decreased (1.073728 --> 1.025497).  Saving model.\n",
      "Epoch: 3 \tTraining Loss: 1.006022 \tValidation Loss: 0.976844\n",
      "Validation loss decreased (1.025497 --> 0.976844).  Saving model.\n",
      "Epoch: 4 \tTraining Loss: 0.963434 \tValidation Loss: 0.946867\n",
      "Validation loss decreased (0.976844 --> 0.946867).  Saving model.\n",
      "Epoch: 5 \tTraining Loss: 0.933874 \tValidation Loss: 0.926890\n",
      "Validation loss decreased (0.946867 --> 0.926890).  Saving model.\n",
      "Epoch: 6 \tTraining Loss: 0.916045 \tValidation Loss: 0.917052\n",
      "Validation loss decreased (0.926890 --> 0.917052).  Saving model.\n",
      "Epoch: 7 \tTraining Loss: 0.903279 \tValidation Loss: 0.910322\n",
      "Validation loss decreased (0.917052 --> 0.910322).  Saving model.\n",
      "Epoch: 8 \tTraining Loss: 0.893837 \tValidation Loss: 0.905126\n",
      "Validation loss decreased (0.910322 --> 0.905126).  Saving model.\n",
      "Epoch: 9 \tTraining Loss: 0.885495 \tValidation Loss: 0.901179\n",
      "Validation loss decreased (0.905126 --> 0.901179).  Saving model.\n",
      "Epoch: 10 \tTraining Loss: 0.877040 \tValidation Loss: 0.898529\n",
      "Validation loss decreased (0.901179 --> 0.898529).  Saving model.\n",
      "Epoch: 11 \tTraining Loss: 0.869125 \tValidation Loss: 0.896469\n",
      "Validation loss decreased (0.898529 --> 0.896469).  Saving model.\n",
      "Epoch: 12 \tTraining Loss: 0.863424 \tValidation Loss: 0.894203\n",
      "Validation loss decreased (0.896469 --> 0.894203).  Saving model.\n",
      "Epoch: 13 \tTraining Loss: 0.856435 \tValidation Loss: 0.892563\n",
      "Validation loss decreased (0.894203 --> 0.892563).  Saving model.\n",
      "Epoch: 14 \tTraining Loss: 0.850456 \tValidation Loss: 0.892035\n",
      "Validation loss decreased (0.892563 --> 0.892035).  Saving model.\n",
      "Epoch: 15 \tTraining Loss: 0.845546 \tValidation Loss: 0.891301\n",
      "Validation loss decreased (0.892035 --> 0.891301).  Saving model.\n",
      "Epoch: 16 \tTraining Loss: 0.841827 \tValidation Loss: 0.890491\n",
      "Validation loss decreased (0.891301 --> 0.890491).  Saving model.\n",
      "Epoch: 17 \tTraining Loss: 0.836847 \tValidation Loss: 0.889972\n",
      "Validation loss decreased (0.890491 --> 0.889972).  Saving model.\n",
      "Epoch: 18 \tTraining Loss: 0.833986 \tValidation Loss: 0.889625\n",
      "Validation loss decreased (0.889972 --> 0.889625).  Saving model.\n",
      "Epoch: 19 \tTraining Loss: 0.827810 \tValidation Loss: 0.890709\n",
      "Epoch: 20 \tTraining Loss: 0.823804 \tValidation Loss: 0.889798\n",
      "Epoch: 21 \tTraining Loss: 0.818352 \tValidation Loss: 0.890212\n",
      "Epoch: 22 \tTraining Loss: 0.815056 \tValidation Loss: 0.890729\n",
      "Epoch: 23 \tTraining Loss: 0.810092 \tValidation Loss: 0.890225\n",
      "Epoch: 24 \tTraining Loss: 0.807175 \tValidation Loss: 0.891439\n",
      "Epoch: 25 \tTraining Loss: 0.802081 \tValidation Loss: 0.891588\n",
      "Epoch: 26 \tTraining Loss: 0.796496 \tValidation Loss: 0.894158\n",
      "Epoch: 27 \tTraining Loss: 0.792556 \tValidation Loss: 0.891479\n",
      "Epoch: 28 \tTraining Loss: 0.788042 \tValidation Loss: 0.893162\n",
      "Epoch: 29 \tTraining Loss: 0.781557 \tValidation Loss: 0.895378\n",
      "Epoch: 30 \tTraining Loss: 0.778935 \tValidation Loss: 0.896075\n",
      "Epoch: 31 \tTraining Loss: 0.776435 \tValidation Loss: 0.896638\n",
      "Epoch: 32 \tTraining Loss: 0.770045 \tValidation Loss: 0.896704\n",
      "Epoch: 33 \tTraining Loss: 0.766829 \tValidation Loss: 0.897576\n",
      "Epoch: 34 \tTraining Loss: 0.760548 \tValidation Loss: 0.897785\n",
      "Epoch: 35 \tTraining Loss: 0.754748 \tValidation Loss: 0.900176\n",
      "Epoch: 36 \tTraining Loss: 0.749900 \tValidation Loss: 0.902023\n",
      "Epoch: 37 \tTraining Loss: 0.742449 \tValidation Loss: 0.901801\n",
      "Epoch: 38 \tTraining Loss: 0.739045 \tValidation Loss: 0.905300\n",
      "Epoch: 39 \tTraining Loss: 0.731256 \tValidation Loss: 0.907159\n",
      "Epoch: 40 \tTraining Loss: 0.728682 \tValidation Loss: 0.907910\n",
      "Epoch: 41 \tTraining Loss: 0.723608 \tValidation Loss: 0.908840\n",
      "Epoch: 42 \tTraining Loss: 0.715805 \tValidation Loss: 0.912593\n",
      "Epoch: 43 \tTraining Loss: 0.710362 \tValidation Loss: 0.912145\n",
      "Epoch: 44 \tTraining Loss: 0.706548 \tValidation Loss: 0.915670\n",
      "Epoch: 45 \tTraining Loss: 0.697351 \tValidation Loss: 0.919396\n",
      "Epoch: 46 \tTraining Loss: 0.692043 \tValidation Loss: 0.919634\n",
      "Epoch: 47 \tTraining Loss: 0.688700 \tValidation Loss: 0.920743\n",
      "Epoch: 48 \tTraining Loss: 0.680741 \tValidation Loss: 0.923606\n",
      "Epoch: 49 \tTraining Loss: 0.672898 \tValidation Loss: 0.931494\n",
      "Epoch: 50 \tTraining Loss: 0.666525 \tValidation Loss: 0.930815\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "validn_min_loss = np.Inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    validn_loss = 0.0\n",
    "    \n",
    "    fnn_model_concat.train()\n",
    "    for rev, rev_class in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = fnn_model_concat(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*rev.size(0)\n",
    "        \n",
    "    fnn_model_concat.eval()\n",
    "    for rev, rev_class in validn_loader:\n",
    "        output = fnn_model_concat(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        \n",
    "        validn_loss += loss.item()*rev.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_loader)*batch_size)\n",
    "    validn_loss = validn_loss/(len(validn_loader)*batch_size)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, validn_loss))\n",
    "    \n",
    "    if validn_loss <= validn_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model.'.format(validn_min_loss, validn_loss))\n",
    "        torch.save(fnn_model_concat.state_dict(), 'fnn_model_concat.pt')\n",
    "        validn_min_loss = validn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae390cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data_concat, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39f49914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_model_concat = FFNetConcat()\n",
    "fnn_model_concat.load_state_dict(torch.load('fnn_model_concat.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4eec6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FNN using first 10 concatenatead Word2Vec vectors: 0.5769166666666666\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of FNN using first 10 concatenatead Word2Vec vectors:',str(accuracy(fnn_model_concat, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5988a",
   "metadata": {},
   "source": [
    "#### Considering average Word2Vec vectors, the Feedforward Neural Network outperformed the single perceptron, and gave comparable results to the Support Vector Machine.\n",
    "\n",
    "It outperforms single perceptron due to it's larger network of hidden layers and nodes, which enables better learning across epochs to classify the data.\n",
    "The linear SVM gave similar results, which might mean that the data was linearly separable, hence a comparable accuracy.\n",
    "\n",
    "#### The first 10 concatenated Word2Vec FNN gave much inferior results compared to SVM, and slightly inferior or similar results to the single perceptron.\n",
    "\n",
    "This could be the case as the first 10 words being concatenated do not necessarily have all the information needed to conclude the sentiment of the review, resulting in likely wrong classifications. Also, the number of input features in this case become too large, which could sometimes inhibit efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d2065",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00727aee",
   "metadata": {},
   "source": [
    "### (a) Simple RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b32b9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataRNN:\n",
    "    \n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        curr_review = self.reviews.iloc[i]['review_body']\n",
    "        curr_review = curr_review.replace(',','')\n",
    "        curr_review = curr_review.replace('.','')\n",
    "        curr_review = curr_review.split()\n",
    "        curr_vect = []\n",
    "        word_count = 0\n",
    "        for word in curr_review:\n",
    "            if word_count==20:\n",
    "                break\n",
    "            if word in w2v:\n",
    "                word_count+=1\n",
    "                curr_vect.append(w2v[word])\n",
    "\n",
    "        while word_count<20:\n",
    "            curr_vect.append(np.zeros((300,), dtype=np.float32))\n",
    "            word_count+=1\n",
    "\n",
    "        if len(curr_vect)==0:\n",
    "            curr_vect = np.zeros((20,300,), dtype=np.float32)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "#             curr_vect = curr_vect.flatten()\n",
    "\n",
    "        curr_vect = torch.from_numpy(curr_vect)\n",
    "        \n",
    "        label = self.ratings.iloc[i]['rating_class']\n",
    "        \n",
    "        return curr_vect, label\n",
    "    \n",
    "\n",
    "\n",
    "class TestDataRNN:\n",
    "    \n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        curr_review = self.reviews.iloc[i]['review_body']\n",
    "        curr_review = curr_review.replace(',','')\n",
    "        curr_review = curr_review.replace('.','')\n",
    "        curr_review = curr_review.split()\n",
    "        curr_vect = []\n",
    "        word_count = 0\n",
    "        for word in curr_review:\n",
    "            if word_count==20:\n",
    "                break\n",
    "            if word in w2v:\n",
    "                word_count+=1\n",
    "                curr_vect.append(w2v[word])\n",
    "\n",
    "        while word_count<20:\n",
    "            curr_vect.append(np.zeros((300,),dtype=np.float32))\n",
    "            word_count+=1\n",
    "\n",
    "        if len(curr_vect)==0:\n",
    "            curr_vect = np.zeros((20,300,),dtype=np.float32)\n",
    "        else:\n",
    "            curr_vect = np.array(curr_vect)\n",
    "#             curr_vect = curr_vect.flatten()\n",
    "\n",
    "        curr_vect = torch.from_numpy(curr_vect)\n",
    "        \n",
    "        label = self.ratings.iloc[i]['rating_class']\n",
    "        \n",
    "        return curr_vect, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "124c9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rnn = TrainDataRNN(X_train,Y_train)\n",
    "test_data_rnn = TestDataRNN(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a318333",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "validation_size=0.2\n",
    "\n",
    "num_train = len(train_data_rnn)\n",
    "inds = list(range(num_train))\n",
    "np.random.shuffle(inds)\n",
    "split = int(np.floor(validation_size*num_train))\n",
    "train_idx, valid_idx = inds[split:], inds[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validn_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_data_rnn, batch_size=batch_size, sampler=train_sampler)\n",
    "validn_loader = DataLoader(train_data_rnn, batch_size=batch_size, sampler=validn_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94c5bb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(300, 20, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=300, hidden_size=20, num_layers=1, dropout=0.2, batch_first=True, nonlinearity='tanh')\n",
    "#         self.fc1 = nn.Linear(20,10)\n",
    "        self.fc = nn.Linear(20,3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.rnn(x)\n",
    "        x = self.fc(x[0][:,-1,:])\n",
    "        \n",
    "        return x\n",
    "\n",
    "rnn_model = RNNModel()\n",
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c8daa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0005) #0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1db28f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.089949 \tValidation Loss: 1.012769\n",
      "Validation loss decreased (inf --> 1.012769).  Saving model.\n",
      "Epoch: 2 \tTraining Loss: 0.960192 \tValidation Loss: 0.941047\n",
      "Validation loss decreased (1.012769 --> 0.941047).  Saving model.\n",
      "Epoch: 3 \tTraining Loss: 0.923711 \tValidation Loss: 0.913140\n",
      "Validation loss decreased (0.941047 --> 0.913140).  Saving model.\n",
      "Epoch: 4 \tTraining Loss: 0.902098 \tValidation Loss: 0.899408\n",
      "Validation loss decreased (0.913140 --> 0.899408).  Saving model.\n",
      "Epoch: 5 \tTraining Loss: 0.891008 \tValidation Loss: 0.892349\n",
      "Validation loss decreased (0.899408 --> 0.892349).  Saving model.\n",
      "Epoch: 6 \tTraining Loss: 0.882429 \tValidation Loss: 0.907064\n",
      "Epoch: 7 \tTraining Loss: 0.876262 \tValidation Loss: 0.882344\n",
      "Validation loss decreased (0.892349 --> 0.882344).  Saving model.\n",
      "Epoch: 8 \tTraining Loss: 0.870632 \tValidation Loss: 0.877208\n",
      "Validation loss decreased (0.882344 --> 0.877208).  Saving model.\n",
      "Epoch: 9 \tTraining Loss: 0.865073 \tValidation Loss: 0.871079\n",
      "Validation loss decreased (0.877208 --> 0.871079).  Saving model.\n",
      "Epoch: 10 \tTraining Loss: 0.858518 \tValidation Loss: 0.867902\n",
      "Validation loss decreased (0.871079 --> 0.867902).  Saving model.\n",
      "Epoch: 11 \tTraining Loss: 0.854750 \tValidation Loss: 0.865163\n",
      "Validation loss decreased (0.867902 --> 0.865163).  Saving model.\n",
      "Epoch: 12 \tTraining Loss: 0.850466 \tValidation Loss: 0.875081\n",
      "Epoch: 13 \tTraining Loss: 0.847763 \tValidation Loss: 0.867676\n",
      "Epoch: 14 \tTraining Loss: 0.843715 \tValidation Loss: 0.864318\n",
      "Validation loss decreased (0.865163 --> 0.864318).  Saving model.\n",
      "Epoch: 15 \tTraining Loss: 0.840204 \tValidation Loss: 0.857334\n",
      "Validation loss decreased (0.864318 --> 0.857334).  Saving model.\n",
      "Epoch: 16 \tTraining Loss: 0.835581 \tValidation Loss: 0.855333\n",
      "Validation loss decreased (0.857334 --> 0.855333).  Saving model.\n",
      "Epoch: 17 \tTraining Loss: 0.834009 \tValidation Loss: 0.851668\n",
      "Validation loss decreased (0.855333 --> 0.851668).  Saving model.\n",
      "Epoch: 18 \tTraining Loss: 0.827520 \tValidation Loss: 0.850193\n",
      "Validation loss decreased (0.851668 --> 0.850193).  Saving model.\n",
      "Epoch: 19 \tTraining Loss: 0.825016 \tValidation Loss: 0.854748\n",
      "Epoch: 20 \tTraining Loss: 0.822885 \tValidation Loss: 0.849054\n",
      "Validation loss decreased (0.850193 --> 0.849054).  Saving model.\n",
      "Epoch: 21 \tTraining Loss: 0.817344 \tValidation Loss: 0.849254\n",
      "Epoch: 22 \tTraining Loss: 0.815093 \tValidation Loss: 0.850057\n",
      "Epoch: 23 \tTraining Loss: 0.812992 \tValidation Loss: 0.860026\n",
      "Epoch: 24 \tTraining Loss: 0.808643 \tValidation Loss: 0.868288\n",
      "Epoch: 25 \tTraining Loss: 0.808550 \tValidation Loss: 0.859373\n",
      "Epoch: 26 \tTraining Loss: 0.804825 \tValidation Loss: 0.837849\n",
      "Validation loss decreased (0.849054 --> 0.837849).  Saving model.\n",
      "Epoch: 27 \tTraining Loss: 0.800772 \tValidation Loss: 0.836785\n",
      "Validation loss decreased (0.837849 --> 0.836785).  Saving model.\n",
      "Epoch: 28 \tTraining Loss: 0.800700 \tValidation Loss: 0.836252\n",
      "Validation loss decreased (0.836785 --> 0.836252).  Saving model.\n",
      "Epoch: 29 \tTraining Loss: 0.799270 \tValidation Loss: 0.844637\n",
      "Epoch: 30 \tTraining Loss: 0.796553 \tValidation Loss: 0.835669\n",
      "Validation loss decreased (0.836252 --> 0.835669).  Saving model.\n",
      "Epoch: 31 \tTraining Loss: 0.793185 \tValidation Loss: 0.838292\n",
      "Epoch: 32 \tTraining Loss: 0.791858 \tValidation Loss: 0.833147\n",
      "Validation loss decreased (0.835669 --> 0.833147).  Saving model.\n",
      "Epoch: 33 \tTraining Loss: 0.787463 \tValidation Loss: 0.830324\n",
      "Validation loss decreased (0.833147 --> 0.830324).  Saving model.\n",
      "Epoch: 34 \tTraining Loss: 0.788507 \tValidation Loss: 0.839016\n",
      "Epoch: 35 \tTraining Loss: 0.784315 \tValidation Loss: 0.831281\n",
      "Epoch: 36 \tTraining Loss: 0.784432 \tValidation Loss: 0.826505\n",
      "Validation loss decreased (0.830324 --> 0.826505).  Saving model.\n",
      "Epoch: 37 \tTraining Loss: 0.781208 \tValidation Loss: 0.823141\n",
      "Validation loss decreased (0.826505 --> 0.823141).  Saving model.\n",
      "Epoch: 38 \tTraining Loss: 0.777687 \tValidation Loss: 0.833087\n",
      "Epoch: 39 \tTraining Loss: 0.777156 \tValidation Loss: 0.831896\n",
      "Epoch: 40 \tTraining Loss: 0.776708 \tValidation Loss: 0.821302\n",
      "Validation loss decreased (0.823141 --> 0.821302).  Saving model.\n",
      "Epoch: 41 \tTraining Loss: 0.775184 \tValidation Loss: 0.824356\n",
      "Epoch: 42 \tTraining Loss: 0.771618 \tValidation Loss: 0.827802\n",
      "Epoch: 43 \tTraining Loss: 0.770497 \tValidation Loss: 0.818253\n",
      "Validation loss decreased (0.821302 --> 0.818253).  Saving model.\n",
      "Epoch: 44 \tTraining Loss: 0.769342 \tValidation Loss: 0.832517\n",
      "Epoch: 45 \tTraining Loss: 0.768456 \tValidation Loss: 0.818075\n",
      "Validation loss decreased (0.818253 --> 0.818075).  Saving model.\n",
      "Epoch: 46 \tTraining Loss: 0.763567 \tValidation Loss: 0.822383\n",
      "Epoch: 47 \tTraining Loss: 0.764590 \tValidation Loss: 0.821240\n",
      "Epoch: 48 \tTraining Loss: 0.765457 \tValidation Loss: 0.826085\n",
      "Epoch: 49 \tTraining Loss: 0.759028 \tValidation Loss: 0.818545\n",
      "Epoch: 50 \tTraining Loss: 0.758664 \tValidation Loss: 0.834599\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "validn_min_loss = np.Inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    validn_loss = 0.0\n",
    "    \n",
    "    rnn_model.train()\n",
    "    for rev, rev_class in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_model(rev)\n",
    "        loss = criterion(output,rev_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*rev.size(0)\n",
    "        \n",
    "    rnn_model.eval()\n",
    "    for rev, rev_class in validn_loader:\n",
    "        output = rnn_model(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        \n",
    "        validn_loss += loss.item()*rev.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_loader)*batch_size)\n",
    "    validn_loss = validn_loss/(len(validn_loader)*batch_size)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, validn_loss))\n",
    "    \n",
    "    if validn_loss <= validn_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model.'.format(validn_min_loss, validn_loss))\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pt')\n",
    "        validn_min_loss = validn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4df37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data_rnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "045e7973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = RNNModel()\n",
    "rnn_model.load_state_dict(torch.load('rnn_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2559ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RNN model: 0.63375\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of RNN model:',str(accuracy(rnn_model, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39748c9",
   "metadata": {},
   "source": [
    "#### The RNN model performed better when compared to the FNN where first 10 words are concatenated.\n",
    "\n",
    "This is because RNN takes into account sequential data. Hence, the sequence of the first 20 words (for RNN) is taken into account across time steps, rather than simply passing the concatenated words as input.\n",
    "\n",
    "#### However, the RNN model did not perform as well as the FNN where average of word embeddings was taken.\n",
    "\n",
    "This can be because the average computed word vectors take the entire review into account, while RNN is only taking the first 20 words. Hence, even though RNN takes sequence into account, as only 20 words are passed to it, they may not always be enough to make an accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d400e",
   "metadata": {},
   "source": [
    "### (b) GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "301db8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru): GRU(300, 20, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=300, hidden_size=20, dropout=0.2, batch_first=True, num_layers=1)\n",
    "        self.fc = nn.Linear(20,3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.gru(x)\n",
    "        x = self.fc(x[0][:,-1,:])\n",
    "        \n",
    "        return x\n",
    "\n",
    "gru_model = GRUModel()\n",
    "print(gru_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41aa2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.0005) #0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e00f2a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.038516 \tValidation Loss: 0.933581\n",
      "Validation loss decreased (inf --> 0.933581).  Saving model.\n",
      "Epoch: 2 \tTraining Loss: 0.903905 \tValidation Loss: 0.888318\n",
      "Validation loss decreased (0.933581 --> 0.888318).  Saving model.\n",
      "Epoch: 3 \tTraining Loss: 0.867890 \tValidation Loss: 0.867774\n",
      "Validation loss decreased (0.888318 --> 0.867774).  Saving model.\n",
      "Epoch: 4 \tTraining Loss: 0.834430 \tValidation Loss: 0.830153\n",
      "Validation loss decreased (0.867774 --> 0.830153).  Saving model.\n",
      "Epoch: 5 \tTraining Loss: 0.808288 \tValidation Loss: 0.810311\n",
      "Validation loss decreased (0.830153 --> 0.810311).  Saving model.\n",
      "Epoch: 6 \tTraining Loss: 0.788979 \tValidation Loss: 0.804769\n",
      "Validation loss decreased (0.810311 --> 0.804769).  Saving model.\n",
      "Epoch: 7 \tTraining Loss: 0.776055 \tValidation Loss: 0.793745\n",
      "Validation loss decreased (0.804769 --> 0.793745).  Saving model.\n",
      "Epoch: 8 \tTraining Loss: 0.766220 \tValidation Loss: 0.797796\n",
      "Epoch: 9 \tTraining Loss: 0.756490 \tValidation Loss: 0.784823\n",
      "Validation loss decreased (0.793745 --> 0.784823).  Saving model.\n",
      "Epoch: 10 \tTraining Loss: 0.748626 \tValidation Loss: 0.778417\n",
      "Validation loss decreased (0.784823 --> 0.778417).  Saving model.\n",
      "Epoch: 11 \tTraining Loss: 0.742965 \tValidation Loss: 0.776289\n",
      "Validation loss decreased (0.778417 --> 0.776289).  Saving model.\n",
      "Epoch: 12 \tTraining Loss: 0.735181 \tValidation Loss: 0.773068\n",
      "Validation loss decreased (0.776289 --> 0.773068).  Saving model.\n",
      "Epoch: 13 \tTraining Loss: 0.730078 \tValidation Loss: 0.770102\n",
      "Validation loss decreased (0.773068 --> 0.770102).  Saving model.\n",
      "Epoch: 14 \tTraining Loss: 0.723923 \tValidation Loss: 0.768598\n",
      "Validation loss decreased (0.770102 --> 0.768598).  Saving model.\n",
      "Epoch: 15 \tTraining Loss: 0.718795 \tValidation Loss: 0.780540\n",
      "Epoch: 16 \tTraining Loss: 0.714230 \tValidation Loss: 0.764136\n",
      "Validation loss decreased (0.768598 --> 0.764136).  Saving model.\n",
      "Epoch: 17 \tTraining Loss: 0.708919 \tValidation Loss: 0.765597\n",
      "Epoch: 18 \tTraining Loss: 0.704573 \tValidation Loss: 0.763810\n",
      "Validation loss decreased (0.764136 --> 0.763810).  Saving model.\n",
      "Epoch: 19 \tTraining Loss: 0.700185 \tValidation Loss: 0.762011\n",
      "Validation loss decreased (0.763810 --> 0.762011).  Saving model.\n",
      "Epoch: 20 \tTraining Loss: 0.695881 \tValidation Loss: 0.764175\n",
      "Epoch: 21 \tTraining Loss: 0.692076 \tValidation Loss: 0.760982\n",
      "Validation loss decreased (0.762011 --> 0.760982).  Saving model.\n",
      "Epoch: 22 \tTraining Loss: 0.689217 \tValidation Loss: 0.788832\n",
      "Epoch: 23 \tTraining Loss: 0.685492 \tValidation Loss: 0.759303\n",
      "Validation loss decreased (0.760982 --> 0.759303).  Saving model.\n",
      "Epoch: 24 \tTraining Loss: 0.680907 \tValidation Loss: 0.779431\n",
      "Epoch: 25 \tTraining Loss: 0.676959 \tValidation Loss: 0.767712\n",
      "Epoch: 26 \tTraining Loss: 0.673193 \tValidation Loss: 0.764562\n",
      "Epoch: 27 \tTraining Loss: 0.672352 \tValidation Loss: 0.764215\n",
      "Epoch: 28 \tTraining Loss: 0.668279 \tValidation Loss: 0.765473\n",
      "Epoch: 29 \tTraining Loss: 0.666183 \tValidation Loss: 0.768486\n",
      "Epoch: 30 \tTraining Loss: 0.661448 \tValidation Loss: 0.777031\n",
      "Epoch: 31 \tTraining Loss: 0.660014 \tValidation Loss: 0.765118\n",
      "Epoch: 32 \tTraining Loss: 0.655387 \tValidation Loss: 0.782981\n",
      "Epoch: 33 \tTraining Loss: 0.653596 \tValidation Loss: 0.782616\n",
      "Epoch: 34 \tTraining Loss: 0.650386 \tValidation Loss: 0.777561\n",
      "Epoch: 35 \tTraining Loss: 0.647108 \tValidation Loss: 0.780452\n",
      "Epoch: 36 \tTraining Loss: 0.644511 \tValidation Loss: 0.770543\n",
      "Epoch: 37 \tTraining Loss: 0.642328 \tValidation Loss: 0.783065\n",
      "Epoch: 38 \tTraining Loss: 0.638761 \tValidation Loss: 0.768719\n",
      "Epoch: 39 \tTraining Loss: 0.636086 \tValidation Loss: 0.778961\n",
      "Epoch: 40 \tTraining Loss: 0.635389 \tValidation Loss: 0.778824\n",
      "Epoch: 41 \tTraining Loss: 0.631664 \tValidation Loss: 0.785076\n",
      "Epoch: 42 \tTraining Loss: 0.629104 \tValidation Loss: 0.788477\n",
      "Epoch: 43 \tTraining Loss: 0.626099 \tValidation Loss: 0.783500\n",
      "Epoch: 44 \tTraining Loss: 0.625453 \tValidation Loss: 0.783778\n",
      "Epoch: 45 \tTraining Loss: 0.622703 \tValidation Loss: 0.785431\n",
      "Epoch: 46 \tTraining Loss: 0.620395 \tValidation Loss: 0.804853\n",
      "Epoch: 47 \tTraining Loss: 0.617138 \tValidation Loss: 0.785744\n",
      "Epoch: 48 \tTraining Loss: 0.614747 \tValidation Loss: 0.794525\n",
      "Epoch: 49 \tTraining Loss: 0.613250 \tValidation Loss: 0.803342\n",
      "Epoch: 50 \tTraining Loss: 0.612144 \tValidation Loss: 0.810026\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "validn_min_loss = np.Inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    validn_loss = 0.0\n",
    "    \n",
    "    gru_model.train()\n",
    "    for rev, rev_class in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = gru_model(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*rev.size(0)\n",
    "        \n",
    "    gru_model.eval()\n",
    "    for rev, rev_class in validn_loader:\n",
    "        output = gru_model(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        \n",
    "        validn_loss += loss.item()*rev.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_loader)*batch_size)\n",
    "    validn_loss = validn_loss/(len(validn_loader)*batch_size)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, validn_loss))\n",
    "    \n",
    "    if validn_loss <= validn_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model.'.format(validn_min_loss, validn_loss))\n",
    "        torch.save(gru_model.state_dict(), 'gru_model.pt')\n",
    "        validn_min_loss = validn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7161108",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data_rnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0cfcfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model = GRUModel()\n",
    "gru_model.load_state_dict(torch.load('gru_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ec9acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GRU model: 0.6644166666666667\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of GRU model:',str(accuracy(gru_model, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a97f9d",
   "metadata": {},
   "source": [
    "### (c) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a3a8c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=20, batch_first=True, num_layers=1)\n",
    "        self.fc = nn.Linear(20,3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.lstm(x)\n",
    "        x = self.fc(x[0][:,-1,:])\n",
    "        return x\n",
    "\n",
    "lstm_model = LSTMModel()\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c76f9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7d97561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.054660 \tValidation Loss: 0.962084\n",
      "Validation loss decreased (inf --> 0.962084).  Saving model.\n",
      "Epoch: 2 \tTraining Loss: 0.916386 \tValidation Loss: 0.890870\n",
      "Validation loss decreased (0.962084 --> 0.890870).  Saving model.\n",
      "Epoch: 3 \tTraining Loss: 0.865927 \tValidation Loss: 0.856398\n",
      "Validation loss decreased (0.890870 --> 0.856398).  Saving model.\n",
      "Epoch: 4 \tTraining Loss: 0.839467 \tValidation Loss: 0.839902\n",
      "Validation loss decreased (0.856398 --> 0.839902).  Saving model.\n",
      "Epoch: 5 \tTraining Loss: 0.819968 \tValidation Loss: 0.834164\n",
      "Validation loss decreased (0.839902 --> 0.834164).  Saving model.\n",
      "Epoch: 6 \tTraining Loss: 0.805430 \tValidation Loss: 0.818035\n",
      "Validation loss decreased (0.834164 --> 0.818035).  Saving model.\n",
      "Epoch: 7 \tTraining Loss: 0.792433 \tValidation Loss: 0.811700\n",
      "Validation loss decreased (0.818035 --> 0.811700).  Saving model.\n",
      "Epoch: 8 \tTraining Loss: 0.778912 \tValidation Loss: 0.803525\n",
      "Validation loss decreased (0.811700 --> 0.803525).  Saving model.\n",
      "Epoch: 9 \tTraining Loss: 0.770507 \tValidation Loss: 0.802362\n",
      "Validation loss decreased (0.803525 --> 0.802362).  Saving model.\n",
      "Epoch: 10 \tTraining Loss: 0.760376 \tValidation Loss: 0.794460\n",
      "Validation loss decreased (0.802362 --> 0.794460).  Saving model.\n",
      "Epoch: 11 \tTraining Loss: 0.753032 \tValidation Loss: 0.789983\n",
      "Validation loss decreased (0.794460 --> 0.789983).  Saving model.\n",
      "Epoch: 12 \tTraining Loss: 0.744277 \tValidation Loss: 0.792139\n",
      "Epoch: 13 \tTraining Loss: 0.739289 \tValidation Loss: 0.783583\n",
      "Validation loss decreased (0.789983 --> 0.783583).  Saving model.\n",
      "Epoch: 14 \tTraining Loss: 0.731745 \tValidation Loss: 0.780825\n",
      "Validation loss decreased (0.783583 --> 0.780825).  Saving model.\n",
      "Epoch: 15 \tTraining Loss: 0.726662 \tValidation Loss: 0.792923\n",
      "Epoch: 16 \tTraining Loss: 0.721692 \tValidation Loss: 0.778367\n",
      "Validation loss decreased (0.780825 --> 0.778367).  Saving model.\n",
      "Epoch: 17 \tTraining Loss: 0.715106 \tValidation Loss: 0.776051\n",
      "Validation loss decreased (0.778367 --> 0.776051).  Saving model.\n",
      "Epoch: 18 \tTraining Loss: 0.709075 \tValidation Loss: 0.775830\n",
      "Validation loss decreased (0.776051 --> 0.775830).  Saving model.\n",
      "Epoch: 19 \tTraining Loss: 0.705081 \tValidation Loss: 0.791379\n",
      "Epoch: 20 \tTraining Loss: 0.701301 \tValidation Loss: 0.793347\n",
      "Epoch: 21 \tTraining Loss: 0.698402 \tValidation Loss: 0.776983\n",
      "Epoch: 22 \tTraining Loss: 0.692608 \tValidation Loss: 0.776202\n",
      "Epoch: 23 \tTraining Loss: 0.688274 \tValidation Loss: 0.782623\n",
      "Epoch: 24 \tTraining Loss: 0.683406 \tValidation Loss: 0.781478\n",
      "Epoch: 25 \tTraining Loss: 0.679477 \tValidation Loss: 0.780547\n",
      "Epoch: 26 \tTraining Loss: 0.675285 \tValidation Loss: 0.775818\n",
      "Validation loss decreased (0.775830 --> 0.775818).  Saving model.\n",
      "Epoch: 27 \tTraining Loss: 0.671558 \tValidation Loss: 0.791184\n",
      "Epoch: 28 \tTraining Loss: 0.667842 \tValidation Loss: 0.780065\n",
      "Epoch: 29 \tTraining Loss: 0.664007 \tValidation Loss: 0.786077\n",
      "Epoch: 30 \tTraining Loss: 0.661436 \tValidation Loss: 0.782004\n",
      "Epoch: 31 \tTraining Loss: 0.657897 \tValidation Loss: 0.791772\n",
      "Epoch: 32 \tTraining Loss: 0.654266 \tValidation Loss: 0.794680\n",
      "Epoch: 33 \tTraining Loss: 0.651520 \tValidation Loss: 0.785573\n",
      "Epoch: 34 \tTraining Loss: 0.646743 \tValidation Loss: 0.781005\n",
      "Epoch: 35 \tTraining Loss: 0.644356 \tValidation Loss: 0.798218\n",
      "Epoch: 36 \tTraining Loss: 0.643074 \tValidation Loss: 0.794712\n",
      "Epoch: 37 \tTraining Loss: 0.637636 \tValidation Loss: 0.791926\n",
      "Epoch: 38 \tTraining Loss: 0.634566 \tValidation Loss: 0.798253\n",
      "Epoch: 39 \tTraining Loss: 0.633744 \tValidation Loss: 0.804032\n",
      "Epoch: 40 \tTraining Loss: 0.628735 \tValidation Loss: 0.799521\n",
      "Epoch: 41 \tTraining Loss: 0.625939 \tValidation Loss: 0.807000\n",
      "Epoch: 42 \tTraining Loss: 0.624332 \tValidation Loss: 0.803915\n",
      "Epoch: 43 \tTraining Loss: 0.620834 \tValidation Loss: 0.812560\n",
      "Epoch: 44 \tTraining Loss: 0.618094 \tValidation Loss: 0.806653\n",
      "Epoch: 45 \tTraining Loss: 0.614602 \tValidation Loss: 0.814086\n",
      "Epoch: 46 \tTraining Loss: 0.611640 \tValidation Loss: 0.812193\n",
      "Epoch: 47 \tTraining Loss: 0.608070 \tValidation Loss: 0.816084\n",
      "Epoch: 48 \tTraining Loss: 0.606253 \tValidation Loss: 0.819537\n",
      "Epoch: 49 \tTraining Loss: 0.603171 \tValidation Loss: 0.813443\n",
      "Epoch: 50 \tTraining Loss: 0.604670 \tValidation Loss: 0.833069\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "validn_min_loss = np.Inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    validn_loss = 0.0\n",
    "    \n",
    "    lstm_model.train()\n",
    "    for rev, rev_class in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm_model(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*rev.size(0)\n",
    "        \n",
    "    lstm_model.eval()\n",
    "    for rev, rev_class in validn_loader:\n",
    "        output = lstm_model(rev)\n",
    "        \n",
    "        loss = criterion(output,rev_class)\n",
    "        \n",
    "        validn_loss += loss.item()*rev.size(0)\n",
    "    \n",
    "    train_loss = train_loss/(len(train_loader)*batch_size)\n",
    "    validn_loss = validn_loss/(len(validn_loader)*batch_size)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, validn_loss))\n",
    "    \n",
    "    if validn_loss <= validn_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model.'.format(validn_min_loss, validn_loss))\n",
    "        torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
    "        validn_min_loss = validn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c05f45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data_rnn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3dccc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTMModel()\n",
    "lstm_model.load_state_dict(torch.load('lstm_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25d81386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSTM model: 0.66225\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of LSTM model:',str(accuracy(lstm_model, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85192ca",
   "metadata": {},
   "source": [
    "#### Both the GRU and the LSTM model show a better performance than regular RNN.\n",
    "\n",
    "This is because both GRU and LSTM compute long-term dependencies better than the regular RNN.\n",
    "Hence, the context of a review is better taken into account on these networks.\n",
    "\n",
    "#### Also, when comparing GRU and LSTM, GRU gave slightly better results across MOST of my model runs.\n",
    "\n",
    "This can be because GRU is known to perform better than LSTM when the dataset is relatively small.\n",
    "\n",
    "If the dataset had even longer sequences, and a larger input size, then LSTM would significantly perform better than GRU due to its superior ability to encode long-term sequences.\n",
    "\n",
    "#### Both GRU and LSTM also give better results than both the Feedforward Networks, the one with average Word2Vec vectors, and the one with first 10 words concatenated.\n",
    "\n",
    "Since GRU and LSTM are taking 20 words sequentially and are known to maintain long-term dependencies well, they outperform both types of FNNs. Even if average word vectors were taken, the data would not be as informative as 20 words taken as input sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7ec25",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[1] https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook - Mentioned in HW description PDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
